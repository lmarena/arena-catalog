{
    "captioning": {
        "gemini-2.5-pro": {
            "rating": 1297.037753333735,
            "rating_q975": 1317.8467381698697,
            "rating_q025": 1276.2287684976004
        },
        "gemini-2.5-flash": {
            "rating": 1277.1411084827619,
            "rating_q975": 1301.4494413869018,
            "rating_q025": 1252.832775578622
        },
        "grok-4-0709": {
            "rating": 1236.5991013183536,
            "rating_q975": 1264.307732242675,
            "rating_q025": 1208.8904703940323
        },
        "gpt-5-chat": {
            "rating": 1220.6634365577213,
            "rating_q975": 1246.874218502538,
            "rating_q025": 1194.4526546129046
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1210.4373969098237,
            "rating_q975": 1234.7869475431166,
            "rating_q025": 1186.0878462765309
        },
        "o4-mini-2025-04-16": {
            "rating": 1208.7984372207277,
            "rating_q975": 1232.8900601776713,
            "rating_q025": 1184.706814263784
        },
        "gpt-5-mini-high": {
            "rating": 1207.0905540536048,
            "rating_q975": 1236.4393628980886,
            "rating_q025": 1177.741745209121
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1205.3716183912393,
            "rating_q975": 1242.895887354053,
            "rating_q025": 1167.8473494284256
        },
        "gpt-5-high": {
            "rating": 1203.8162288305386,
            "rating_q975": 1230.8834611451155,
            "rating_q025": 1176.7489965159618
        },
        "o3-2025-04-16": {
            "rating": 1201.5777361405992,
            "rating_q975": 1224.327461166643,
            "rating_q025": 1178.8280111145555
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1212.7592664739907,
            "rating_q025": 1163.9407335260091
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1171.864264688275,
            "rating_q975": 1198.0136687820745,
            "rating_q025": 1145.7148605944753
        },
        "gemma-3-27b-it": {
            "rating": 1151.0743619569,
            "rating_q975": 1182.8516361860084,
            "rating_q025": 1119.2970877277917
        },
        "mistral-medium-2508": {
            "rating": 1146.343908609886,
            "rating_q975": 1175.7583385832281,
            "rating_q025": 1116.929478636544
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1137.3372669426235,
            "rating_q975": 1167.3191955512011,
            "rating_q025": 1107.3553383340459
        },
        "mistral-medium-2505": {
            "rating": 1104.2877090041886,
            "rating_q975": 1143.2055493946682,
            "rating_q025": 1065.369868613709
        },
        "mistral-small-2506": {
            "rating": 1070.0254235193647,
            "rating_q975": 1108.482445052975,
            "rating_q025": 1031.5684019857545
        }
    },
    "chinese": {
        "gemini-2.5-pro": {
            "rating": 1290.6429941761187,
            "rating_q975": 1319.006939535663,
            "rating_q025": 1262.2790488165745
        },
        "gpt-5-chat": {
            "rating": 1288.935132443052,
            "rating_q975": 1319.7150662173328,
            "rating_q025": 1258.1551986687712
        },
        "gemini-2.5-flash": {
            "rating": 1270.7453045802345,
            "rating_q975": 1300.1246445135878,
            "rating_q025": 1241.3659646468811
        },
        "grok-4-0709": {
            "rating": 1252.1508384675067,
            "rating_q975": 1282.7757961154612,
            "rating_q025": 1221.5258808195522
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1247.8041223388532,
            "rating_q975": 1280.6304239635665,
            "rating_q025": 1214.9778207141399
        },
        "gpt-5-high": {
            "rating": 1242.2738376610891,
            "rating_q975": 1272.6318606166574,
            "rating_q025": 1211.915814705521
        },
        "o3-2025-04-16": {
            "rating": 1236.234892135948,
            "rating_q975": 1264.9530333410225,
            "rating_q025": 1207.5167509308735
        },
        "gpt-5-mini-high": {
            "rating": 1231.3627834736408,
            "rating_q975": 1262.3515892351736,
            "rating_q025": 1200.373977712108
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1226.880190374088,
            "rating_q975": 1254.7235355002015,
            "rating_q025": 1199.0368452479745
        },
        "o1-2024-12-17": {
            "rating": 1224.4425018332352,
            "rating_q975": 1282.7981113825347,
            "rating_q025": 1166.0868922839356
        },
        "o4-mini-2025-04-16": {
            "rating": 1213.3822047889803,
            "rating_q975": 1242.9645858174408,
            "rating_q025": 1183.7998237605198
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1218.6762691603778,
            "rating_q025": 1158.023730839622
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1179.2620537312569,
            "rating_q975": 1207.6092251749071,
            "rating_q025": 1150.9148822876066
        },
        "gemini-1.5-pro-002": {
            "rating": 1173.9071293230909,
            "rating_q975": 1208.8357653791506,
            "rating_q025": 1138.9784932670311
        },
        "mistral-medium-2508": {
            "rating": 1170.8717852979453,
            "rating_q975": 1202.6964372314105,
            "rating_q025": 1139.04713336448
        },
        "gpt-4.5-preview-2025-02-27": {
            "rating": 1163.5183922949323,
            "rating_q975": 1226.673742264452,
            "rating_q025": 1100.3630423254126
        },
        "gemma-3-27b-it": {
            "rating": 1153.5884740465838,
            "rating_q975": 1186.2557611479692,
            "rating_q025": 1120.9211869451983
        },
        "mistral-medium-2505": {
            "rating": 1148.2134861148177,
            "rating_q975": 1182.3446271876967,
            "rating_q025": 1114.0823450419387
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1143.9047054626717,
            "rating_q975": 1176.0157741445844,
            "rating_q025": 1111.793636780759
        },
        "mistral-small-2506": {
            "rating": 1139.1736600804406,
            "rating_q975": 1174.00529000652,
            "rating_q025": 1104.3420301543613
        },
        "llama-4-maverick-17b-128e-instruct": {
            "rating": 1125.0833951441273,
            "rating_q975": 1167.4651630480694,
            "rating_q025": 1082.7016272401852
        },
        "gemini-1.5-flash-002": {
            "rating": 1124.2463305727647,
            "rating_q975": 1160.0884801624245,
            "rating_q025": 1088.4041809831049
        },
        "gpt-4o-2024-05-13": {
            "rating": 1119.3334036632054,
            "rating_q975": 1146.4331844372593,
            "rating_q025": 1092.2336228891515
        },
        "claude-3-5-sonnet-20241022": {
            "rating": 1106.2551773676537,
            "rating_q975": 1136.0044952229084,
            "rating_q025": 1076.505859512399
        },
        "qwen2.5-vl-72b-instruct": {
            "rating": 1095.0213210560692,
            "rating_q975": 1148.093742629151,
            "rating_q025": 1041.9488994829874
        },
        "claude-3-5-sonnet-20240620": {
            "rating": 1092.4028046220078,
            "rating_q975": 1120.621791413938,
            "rating_q025": 1064.1838178300777
        },
        "llama-4-scout-17b-16e-instruct": {
            "rating": 1079.525653277046,
            "rating_q975": 1122.7367349520496,
            "rating_q025": 1036.3145716020426
        },
        "pixtral-large-2411": {
            "rating": 1070.6135410931388,
            "rating_q975": 1117.0247675961934,
            "rating_q025": 1024.2023145900841
        },
        "qwen2-vl-72b": {
            "rating": 1058.8653276977425,
            "rating_q975": 1095.9283659091736,
            "rating_q025": 1021.8022894863113
        },
        "internvl2-26b": {
            "rating": 1052.102568285228,
            "rating_q975": 1092.2826246194898,
            "rating_q025": 1011.9225119509662
        },
        "gpt-4-turbo-2024-04-09": {
            "rating": 1032.196729624669,
            "rating_q975": 1062.5032402967058,
            "rating_q025": 1001.8902189526319
        },
        "gpt-4o-mini-2024-07-18": {
            "rating": 1020.1835645578515,
            "rating_q975": 1047.623977732659,
            "rating_q025": 992.7431513830442
        },
        "gemini-1.5-pro-001": {
            "rating": 1017.5182381043728,
            "rating_q975": 1047.2294546765906,
            "rating_q025": 987.8070215321551
        },
        "gemini-2.0-flash-lite-preview-02-05": {
            "rating": 1014.8469771257437,
            "rating_q975": 1073.2692693060233,
            "rating_q025": 956.4246849454642
        },
        "gpt-4o-2024-08-06": {
            "rating": 996.5993319817142,
            "rating_q975": 1041.861395316421,
            "rating_q025": 951.3372686470074
        },
        "qwen2-vl-7b-instruct": {
            "rating": 986.4568143886181,
            "rating_q975": 1024.183994213729,
            "rating_q025": 948.7296345635071
        },
        "claude-3-opus-20240229": {
            "rating": 981.3106006010996,
            "rating_q975": 1011.3509184866696,
            "rating_q025": 951.2702827155297
        },
        "gemini-1.5-flash-8b-001": {
            "rating": 960.827669299126,
            "rating_q975": 997.0522920786001,
            "rating_q025": 924.6030465196519
        },
        "gemini-1.5-flash-001": {
            "rating": 946.9427782254498,
            "rating_q975": 977.5553801530235,
            "rating_q025": 916.3301762978762
        },
        "llama-3.2-vision-90b-instruct": {
            "rating": 939.5643990403788,
            "rating_q975": 974.1474748111308,
            "rating_q025": 904.9813232696268
        },
        "pixtral-12b-2409": {
            "rating": 937.8703168077966,
            "rating_q975": 974.1952889925627,
            "rating_q025": 901.5453446230305
        },
        "internvl2-4b": {
            "rating": 937.5030534398326,
            "rating_q975": 985.5734091459774,
            "rating_q025": 889.4326977336877
        },
        "claude-3-sonnet-20240229": {
            "rating": 929.5297734881915,
            "rating_q975": 960.2534368760339,
            "rating_q025": 898.8061101003491
        },
        "molmo-72b-0924": {
            "rating": 923.5873550824278,
            "rating_q975": 971.9171268317622,
            "rating_q025": 875.2575833330935
        },
        "claude-3-haiku-20240307": {
            "rating": 902.7973826516514,
            "rating_q975": 933.4929218012613,
            "rating_q025": 872.1018435020416
        },
        "llama-3.2-vision-11b-instruct": {
            "rating": 902.0370134473642,
            "rating_q975": 943.0793575881896,
            "rating_q025": 860.9946693065388
        },
        "llava-v1.6-34b": {
            "rating": 901.994245090526,
            "rating_q975": 939.7102632438787,
            "rating_q025": 864.2782269371733
        },
        "molmo-7b-d-0924": {
            "rating": 877.2785764943728,
            "rating_q975": 924.5341397019976,
            "rating_q025": 830.0230132867481
        }
    },
    "creative_writing_vision": {
        "gemini-2.5-flash-preview-09-2025": {
            "rating": 1270.7651037861779,
            "rating_q975": 1305.6717038126737,
            "rating_q025": 1235.858503759682
        },
        "gemini-2.5-pro": {
            "rating": 1268.8421173140391,
            "rating_q975": 1283.5908703737782,
            "rating_q025": 1254.0933642543
        },
        "grok-4-0709": {
            "rating": 1263.8790585970478,
            "rating_q975": 1281.7766066097506,
            "rating_q025": 1245.981510584345
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1257.8532850523138,
            "rating_q975": 1280.8408008863573,
            "rating_q025": 1234.8657692182703
        },
        "claude-opus-4-20250514": {
            "rating": 1254.451772883363,
            "rating_q975": 1286.7731782617834,
            "rating_q025": 1222.1303675049426
        },
        "gemini-2.5-flash": {
            "rating": 1250.760577541288,
            "rating_q975": 1266.9927373128985,
            "rating_q025": 1234.5284177696776
        },
        "gpt-5-chat": {
            "rating": 1242.6897285093264,
            "rating_q975": 1260.250533032414,
            "rating_q025": 1225.1289239862388
        },
        "qwen3-vl-235b-a22b-thinking": {
            "rating": 1228.802126131291,
            "rating_q975": 1266.5826590228696,
            "rating_q025": 1191.0215932397125
        },
        "qwen3-vl-235b-a22b-instruct": {
            "rating": 1227.5319544243966,
            "rating_q975": 1260.470480214265,
            "rating_q025": 1194.5934286345282
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1223.1390632162088,
            "rating_q975": 1239.894660029706,
            "rating_q025": 1206.3834664027115
        },
        "gemini-2.5-flash-lite-preview-09-2025-no-thinking": {
            "rating": 1216.2328619977543,
            "rating_q975": 1251.3241009162182,
            "rating_q025": 1181.1416230792904
        },
        "mistral-medium-2508": {
            "rating": 1215.8067030975399,
            "rating_q975": 1234.5680236077026,
            "rating_q025": 1197.0453825873772
        },
        "o3-2025-04-16": {
            "rating": 1204.533181566127,
            "rating_q975": 1219.8831113595718,
            "rating_q025": 1189.1832517726823
        },
        "claude-sonnet-4-20250514": {
            "rating": 1194.1187469204,
            "rating_q975": 1233.2133660526779,
            "rating_q025": 1155.0241277881223
        },
        "gpt-5-high": {
            "rating": 1193.1812616511074,
            "rating_q975": 1210.3987627722238,
            "rating_q025": 1175.963760529991
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1205.4756845583065,
            "rating_q025": 1171.2243154416933
        },
        "mistral-small-2506": {
            "rating": 1185.225961336601,
            "rating_q975": 1212.9864387710038,
            "rating_q025": 1157.465483902198
        },
        "gpt-5-mini-high": {
            "rating": 1184.2989633072903,
            "rating_q975": 1203.0999988054452,
            "rating_q025": 1165.4979278091353
        },
        "qwen-vl-max-2025-08-13": {
            "rating": 1183.0994366925138,
            "rating_q975": 1225.239804999413,
            "rating_q025": 1140.9590683856145
        },
        "gemma-3-27b-it": {
            "rating": 1178.7628306609504,
            "rating_q975": 1200.9493951569002,
            "rating_q025": 1156.5762661650006
        },
        "hunyuan-vision-1.5-thinking": {
            "rating": 1175.854154607789,
            "rating_q975": 1211.3260339244387,
            "rating_q025": 1140.3822752911394
        },
        "mistral-medium-2505": {
            "rating": 1168.9541712663781,
            "rating_q975": 1193.3055181296868,
            "rating_q025": 1144.6028244030695
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1164.8385041399683,
            "rating_q975": 1182.1348856946086,
            "rating_q025": 1147.5421225853281
        },
        "o4-mini-2025-04-16": {
            "rating": 1163.6884039403462,
            "rating_q975": 1180.3156866726479,
            "rating_q025": 1147.0611212080446
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1143.930705829378,
            "rating_q975": 1165.3741730636705,
            "rating_q025": 1122.4872385950855
        },
        "llama-4-scout-17b-16e-instruct": {
            "rating": 1125.5065896610183,
            "rating_q975": 1161.8145249524769,
            "rating_q025": 1089.1986543695598
        },
        "gpt-5-nano-high": {
            "rating": 1116.2699884339258,
            "rating_q975": 1156.5602451024854,
            "rating_q025": 1075.9797317653663
        },
        "llama-4-maverick-17b-128e-instruct": {
            "rating": 1113.155279933741,
            "rating_q975": 1150.5892281542988,
            "rating_q025": 1075.7213317131832
        }
    },
    "diagram": {
        "gemini-2.5-pro": {
            "rating": 1285.6106046101413,
            "rating_q975": 1296.5524825067414,
            "rating_q025": 1274.6687267135412
        },
        "qwen-vl-max-2025-08-13": {
            "rating": 1273.8564883180973,
            "rating_q975": 1304.019220930257,
            "rating_q025": 1243.6937557059375
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1257.2092380621746,
            "rating_q975": 1271.0730222609263,
            "rating_q025": 1243.3454538634228
        },
        "gpt-5-chat": {
            "rating": 1253.798397229778,
            "rating_q975": 1267.858499525452,
            "rating_q025": 1239.7382949341038
        },
        "gemini-2.5-flash-preview-09-2025": {
            "rating": 1245.7982583328417,
            "rating_q975": 1268.7372493642004,
            "rating_q025": 1222.859267301483
        },
        "qwen3-vl-235b-a22b-instruct": {
            "rating": 1244.4921403316139,
            "rating_q975": 1266.9724560627974,
            "rating_q025": 1222.0118246004304
        },
        "gpt-5-high": {
            "rating": 1240.6845822813125,
            "rating_q975": 1254.356565581783,
            "rating_q025": 1227.012598980842
        },
        "hunyuan-vision-1.5-thinking": {
            "rating": 1236.2836430655736,
            "rating_q975": 1261.0628736912013,
            "rating_q025": 1211.504412439946
        },
        "gemini-2.5-flash": {
            "rating": 1236.2024749764612,
            "rating_q975": 1247.5829663129089,
            "rating_q025": 1224.8219836400135
        },
        "o3-2025-04-16": {
            "rating": 1231.609966427077,
            "rating_q975": 1243.2732402855343,
            "rating_q025": 1219.94669256862
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1223.440158657509,
            "rating_q975": 1235.6637437953625,
            "rating_q025": 1211.2165735196556
        },
        "qwen3-vl-235b-a22b-thinking": {
            "rating": 1218.9362884879679,
            "rating_q975": 1243.5603952294503,
            "rating_q025": 1194.3121817464855
        },
        "claude-sonnet-4-20250514-thinking-32k": {
            "rating": 1209.6453736490355,
            "rating_q975": 1245.2236095451037,
            "rating_q025": 1174.0671377529673
        },
        "gemini-2.5-flash-lite-preview-09-2025-no-thinking": {
            "rating": 1205.812560210924,
            "rating_q975": 1228.5826779639938,
            "rating_q025": 1183.0424424578541
        },
        "grok-4-0709": {
            "rating": 1203.1241009754172,
            "rating_q975": 1216.5473407228558,
            "rating_q025": 1189.7008612279785
        },
        "gpt-5-mini-high": {
            "rating": 1200.165262534082,
            "rating_q975": 1214.472575704687,
            "rating_q025": 1185.8579493634772
        },
        "o4-mini-2025-04-16": {
            "rating": 1199.3952019697203,
            "rating_q975": 1211.361257962676,
            "rating_q025": 1187.4291459767646
        },
        "claude-opus-4-20250514-thinking-16k": {
            "rating": 1195.8416476779398,
            "rating_q975": 1229.5963303722242,
            "rating_q025": 1162.0869649836554
        },
        "step-1o-turbo-202506": {
            "rating": 1189.9455827012325,
            "rating_q975": 1217.71884201535,
            "rating_q025": 1162.172323387115
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1200.751094018732,
            "rating_q025": 1175.948905981268
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1185.2549642085494,
            "rating_q975": 1197.9070349050276,
            "rating_q025": 1172.6028935120712
        },
        "claude-opus-4-20250514": {
            "rating": 1184.3784394426002,
            "rating_q975": 1208.5675732736454,
            "rating_q025": 1160.189305611555
        },
        "claude-sonnet-4-20250514": {
            "rating": 1180.5608130419846,
            "rating_q975": 1207.7877343078821,
            "rating_q025": 1153.333891776087
        },
        "claude-3-7-sonnet-20250219-thinking-32k": {
            "rating": 1179.8103916823202,
            "rating_q975": 1212.2461226352093,
            "rating_q025": 1147.374660729431
        },
        "mistral-medium-2508": {
            "rating": 1166.324783287676,
            "rating_q975": 1181.396513924046,
            "rating_q025": 1151.2530526513058
        },
        "glm-4.5v": {
            "rating": 1164.9585644937542,
            "rating_q975": 1192.6634031603419,
            "rating_q025": 1137.2537258271666
        },
        "gemma-3-27b-it": {
            "rating": 1154.5100728864572,
            "rating_q975": 1168.6357631351011,
            "rating_q025": 1140.3843826378134
        },
        "mistral-medium-2505": {
            "rating": 1153.0527254677322,
            "rating_q975": 1167.9684548833764,
            "rating_q025": 1138.136996052088
        },
        "step-3": {
            "rating": 1149.8545013201237,
            "rating_q975": 1177.9792931377244,
            "rating_q025": 1121.729709502523
        },
        "hunyuan-large-vision": {
            "rating": 1144.990505624785,
            "rating_q975": 1177.8141195988032,
            "rating_q025": 1112.166891650767
        },
        "llama-4-maverick-17b-128e-instruct": {
            "rating": 1136.9324712469129,
            "rating_q975": 1155.0292662713432,
            "rating_q025": 1118.8356762224826
        },
        "gpt-5-nano-high": {
            "rating": 1135.9689002943471,
            "rating_q975": 1161.9836899643349,
            "rating_q025": 1109.9541106243594
        },
        "claude-3-5-sonnet-20241022": {
            "rating": 1130.5916895038586,
            "rating_q975": 1160.9287688790396,
            "rating_q025": 1100.2546101286775
        },
        "llama-4-scout-17b-16e-instruct": {
            "rating": 1128.043552482433,
            "rating_q975": 1147.2219794465739,
            "rating_q025": 1108.865125518292
        },
        "mistral-small-2506": {
            "rating": 1127.4910821289527,
            "rating_q975": 1144.5592660215316,
            "rating_q025": 1110.4228982363738
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1124.7862984795365,
            "rating_q975": 1139.6286328942922,
            "rating_q025": 1109.9439640647809
        },
        "claude-3-5-haiku-20241022": {
            "rating": 1088.1163302650561,
            "rating_q975": 1119.3050674052604,
            "rating_q025": 1056.927593124852
        }
    },
    "english": {
        "gemini-2.5-pro": {
            "rating": 1269.9891383417862,
            "rating_q975": 1280.165909980925,
            "rating_q025": 1259.8123667026473
        },
        "gemini-2.5-flash-preview-09-2025": {
            "rating": 1247.8136601074054,
            "rating_q975": 1265.1361233704017,
            "rating_q025": 1230.4911968444092
        },
        "qwen3-vl-235b-a22b-instruct": {
            "rating": 1245.1613387367556,
            "rating_q975": 1261.4279906317838,
            "rating_q025": 1228.8946868417274
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1242.9211110327255,
            "rating_q975": 1252.478283677783,
            "rating_q025": 1233.363938387668
        },
        "gemini-2.5-flash": {
            "rating": 1242.0797956860456,
            "rating_q975": 1252.3810118078827,
            "rating_q025": 1231.7785795642085
        },
        "gpt-5-chat": {
            "rating": 1234.4848139187902,
            "rating_q975": 1246.1693051650286,
            "rating_q025": 1222.8003226725518
        },
        "hunyuan-vision-1.5-thinking": {
            "rating": 1231.9914266809747,
            "rating_q975": 1249.4202870985273,
            "rating_q025": 1214.562566263422
        },
        "qwen-vl-max-2025-08-13": {
            "rating": 1229.8033481999944,
            "rating_q975": 1247.1875691281468,
            "rating_q025": 1212.419127271842
        },
        "o3-2025-04-16": {
            "rating": 1224.8377921721485,
            "rating_q975": 1234.7343920672868,
            "rating_q025": 1214.9411922770103
        },
        "gpt-5-high": {
            "rating": 1218.580610853105,
            "rating_q975": 1230.0925307411264,
            "rating_q025": 1207.0686909650838
        },
        "grok-4-0709": {
            "rating": 1217.9841504612473,
            "rating_q975": 1229.3359470579364,
            "rating_q025": 1206.6323538645581
        },
        "qwen3-vl-235b-a22b-thinking": {
            "rating": 1216.6625618187256,
            "rating_q975": 1234.6773864847191,
            "rating_q025": 1198.6477371527321
        },
        "gemini-2.5-flash-lite-preview-09-2025-no-thinking": {
            "rating": 1215.546991478981,
            "rating_q975": 1232.35251459859,
            "rating_q025": 1198.741468359372
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1214.2330964133807,
            "rating_q975": 1224.2498687715536,
            "rating_q025": 1204.216324055208
        },
        "gpt-5-mini-high": {
            "rating": 1213.1813946018497,
            "rating_q975": 1224.7843058801532,
            "rating_q025": 1201.5784833235462
        },
        "step-1o-turbo-202506": {
            "rating": 1207.5359489681948,
            "rating_q975": 1227.2547232768948,
            "rating_q025": 1187.8171746594949
        },
        "o4-mini-2025-04-16": {
            "rating": 1197.9241073433263,
            "rating_q975": 1208.2161892919185,
            "rating_q025": 1187.6320253947342
        },
        "step-3": {
            "rating": 1193.922979747534,
            "rating_q975": 1210.4791612197153,
            "rating_q025": 1177.366798275353
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1192.3800369411792,
            "rating_q975": 1202.8197167041358,
            "rating_q025": 1181.9403571782227
        },
        "hunyuan-large-vision": {
            "rating": 1191.2110551463647,
            "rating_q975": 1213.64425963842,
            "rating_q025": 1168.7778506543095
        },
        "gpt-4.5-preview-2025-02-27": {
            "rating": 1189.1628837837798,
            "rating_q975": 1204.935440064165,
            "rating_q025": 1173.3903275033945
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1199.2445500356355,
            "rating_q025": 1177.4554499643643
        },
        "glm-4.5v": {
            "rating": 1186.1193296740828,
            "rating_q975": 1202.7968795280726,
            "rating_q025": 1169.441779820093
        },
        "claude-sonnet-4-20250514-thinking-32k": {
            "rating": 1185.0103950882133,
            "rating_q975": 1209.1492501528185,
            "rating_q025": 1160.871540023608
        },
        "gemma-3-27b-it": {
            "rating": 1184.3917590866854,
            "rating_q975": 1195.664780109035,
            "rating_q025": 1173.1187380643357
        },
        "claude-opus-4-20250514": {
            "rating": 1180.2237194196023,
            "rating_q975": 1198.165667418269,
            "rating_q025": 1162.2817714209355
        },
        "mistral-medium-2508": {
            "rating": 1178.4659768252632,
            "rating_q975": 1190.3784926501694,
            "rating_q025": 1166.553461000357
        },
        "claude-opus-4-20250514-thinking-16k": {
            "rating": 1177.9431035086195,
            "rating_q975": 1201.7352546049165,
            "rating_q025": 1154.1509524123226
        },
        "mistral-medium-2505": {
            "rating": 1174.4960709431357,
            "rating_q975": 1186.0107943816663,
            "rating_q025": 1162.981347504605
        },
        "gpt-5-nano-high": {
            "rating": 1166.7747430930824,
            "rating_q975": 1182.7071714343056,
            "rating_q025": 1150.8423147518592
        },
        "claude-sonnet-4-20250514": {
            "rating": 1165.074279422804,
            "rating_q975": 1185.7919972936709,
            "rating_q025": 1144.3565615519371
        },
        "claude-3-7-sonnet-20250219-thinking-32k": {
            "rating": 1163.6094754810338,
            "rating_q975": 1185.3807729170944,
            "rating_q025": 1141.8381780449731
        },
        "o1-2024-12-17": {
            "rating": 1160.7895306721034,
            "rating_q975": 1174.5334417500378,
            "rating_q025": 1147.045619594169
        },
        "mistral-small-2506": {
            "rating": 1158.3609217472517,
            "rating_q975": 1170.6778262782395,
            "rating_q025": 1146.044017216264
        },
        "gemini-1.5-pro-002": {
            "rating": 1157.2644286286218,
            "rating_q975": 1167.3361196705648,
            "rating_q025": 1147.1927375866787
        },
        "qwen2.5-vl-32b-instruct": {
            "rating": 1156.968203374024,
            "rating_q975": 1176.0862174214324,
            "rating_q025": 1137.8501893266155
        },
        "gemini-1.5-flash-002": {
            "rating": 1155.4185205351923,
            "rating_q975": 1166.5075120220822,
            "rating_q025": 1144.3295290483024
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1151.5257805824046,
            "rating_q975": 1162.9834676259964,
            "rating_q025": 1140.068093538813
        },
        "llama-4-maverick-17b-128e-instruct": {
            "rating": 1145.6834555906191,
            "rating_q975": 1158.0727098779298,
            "rating_q025": 1133.2942013033085
        },
        "step-1o-vision-32k-highres": {
            "rating": 1145.4461864232226,
            "rating_q975": 1160.744936456198,
            "rating_q025": 1130.147436390247
        },
        "gpt-4o-2024-05-13": {
            "rating": 1145.3879180462893,
            "rating_q975": 1154.8263117925749,
            "rating_q025": 1135.9495243000038
        },
        "llama-4-scout-17b-16e-instruct": {
            "rating": 1136.599727786977,
            "rating_q975": 1149.6910195260575,
            "rating_q025": 1123.5084360478963
        },
        "claude-3-5-sonnet-20241022": {
            "rating": 1120.5086831385672,
            "rating_q975": 1129.5661651639575,
            "rating_q025": 1111.451201113177
        },
        "gemini-2.0-flash-lite-preview-02-05": {
            "rating": 1119.4939004483226,
            "rating_q975": 1132.1951519842662,
            "rating_q025": 1106.792648912379
        },
        "claude-3-5-sonnet-20240620": {
            "rating": 1117.285339635021,
            "rating_q975": 1128.6783316842216,
            "rating_q025": 1105.8923475858205
        },
        "qwen2.5-vl-72b-instruct": {
            "rating": 1113.261738582641,
            "rating_q975": 1126.0925737707928,
            "rating_q025": 1100.4309033944894
        },
        "claude-3-5-haiku-20241022": {
            "rating": 1102.6768304358961,
            "rating_q975": 1125.315698517004,
            "rating_q025": 1080.0379623547883
        },
        "gpt-4.1-nano-2025-04-14": {
            "rating": 1099.7264231205031,
            "rating_q975": 1126.2664231630715,
            "rating_q025": 1073.1864230779347
        },
        "pixtral-large-2411": {
            "rating": 1098.7949309280302,
            "rating_q975": 1109.879685116308,
            "rating_q025": 1087.7101767397523
        },
        "gpt-4-turbo-2024-04-09": {
            "rating": 1094.9993504151214,
            "rating_q975": 1107.394526237292,
            "rating_q025": 1082.6041745929508
        },
        "gemini-1.5-pro-001": {
            "rating": 1087.9563529555985,
            "rating_q975": 1100.1001177643068,
            "rating_q025": 1075.8125881468902
        },
        "gpt-4o-mini-2024-07-18": {
            "rating": 1071.0521870747025,
            "rating_q975": 1080.1007176968205,
            "rating_q025": 1062.0036564525844
        },
        "qwen-vl-max-1119": {
            "rating": 1071.035203151327,
            "rating_q975": 1092.0943470815143,
            "rating_q025": 1049.9760592211398
        },
        "gpt-4o-2024-08-06": {
            "rating": 1070.4310576456055,
            "rating_q975": 1085.7823434421064,
            "rating_q025": 1055.0797718491046
        },
        "step-1v-32k": {
            "rating": 1068.5503093446532,
            "rating_q975": 1089.724248892384,
            "rating_q025": 1047.3763697969225
        },
        "gemini-1.5-flash-8b-001": {
            "rating": 1068.4843439375209,
            "rating_q975": 1080.435340634328,
            "rating_q025": 1056.5333472407137
        },
        "qwen2-vl-72b": {
            "rating": 1051.7724242184804,
            "rating_q975": 1063.5599445664707,
            "rating_q025": 1039.9849038704901
        },
        "molmo-72b-0924": {
            "rating": 1043.426779395209,
            "rating_q975": 1059.5094578101048,
            "rating_q025": 1027.3441009803132
        },
        "internvl2-26b": {
            "rating": 1035.5800742685242,
            "rating_q975": 1049.7463317318318,
            "rating_q025": 1021.4138168052166
        },
        "gemini-1.5-flash-001": {
            "rating": 1025.6294028528907,
            "rating_q975": 1038.1804864853225,
            "rating_q025": 1013.0783192204589
        },
        "pixtral-12b-2409": {
            "rating": 1025.0379015600197,
            "rating_q975": 1035.822284735095,
            "rating_q025": 1014.2535183849442
        },
        "hunyuan-standard-vision-2024-12-31": {
            "rating": 1018.063662673249,
            "rating_q975": 1045.4608142428915,
            "rating_q025": 990.6665111036066
        },
        "claude-3-opus-20240229": {
            "rating": 1016.8864147010577,
            "rating_q975": 1028.696712025949,
            "rating_q025": 1005.0761173761665
        },
        "llama-3.2-vision-90b-instruct": {
            "rating": 1011.2414432170374,
            "rating_q975": 1021.8081083072141,
            "rating_q025": 1000.6747781268607
        },
        "molmo-7b-d-0924": {
            "rating": 1010.0391726167478,
            "rating_q975": 1027.0270499914993,
            "rating_q025": 993.0512952419963
        },
        "yi-vision": {
            "rating": 1005.8638258039969,
            "rating_q975": 1027.7708925399133,
            "rating_q025": 983.9567590680803
        },
        "qwen2-vl-7b-instruct": {
            "rating": 1004.180195169042,
            "rating_q975": 1016.3880092367867,
            "rating_q025": 991.9723811012973
        },
        "amazon-nova-lite-v1.0": {
            "rating": 997.1215161579148,
            "rating_q975": 1015.9210588490483,
            "rating_q025": 978.3219734667813
        },
        "c4ai-aya-vision-32b": {
            "rating": 996.6306300378785,
            "rating_q975": 1026.4283291689167,
            "rating_q025": 966.8329309068404
        },
        "llama-3.2-vision-11b-instruct": {
            "rating": 995.0823827235255,
            "rating_q975": 1008.4065452873863,
            "rating_q025": 981.7582201596647
        },
        "amazon-nova-pro-v1.0": {
            "rating": 985.7851618837975,
            "rating_q975": 1003.4646248373215,
            "rating_q025": 968.1056989302735
        },
        "internvl2-4b": {
            "rating": 984.1437865076679,
            "rating_q975": 999.0126654197568,
            "rating_q025": 969.274907595579
        },
        "claude-3-sonnet-20240229": {
            "rating": 978.8050138525929,
            "rating_q975": 991.3069344137458,
            "rating_q025": 966.30309329144
        },
        "llava-v1.6-34b": {
            "rating": 968.7485191489066,
            "rating_q975": 983.3114553675226,
            "rating_q025": 954.1855829302905
        },
        "cogvlm2-llama3-chat-19b": {
            "rating": 967.3212241422345,
            "rating_q975": 986.0712304971472,
            "rating_q025": 948.5712177873219
        },
        "llava-onevision-qwen2-72b-ov": {
            "rating": 960.258416565342,
            "rating_q975": 981.3097051908395,
            "rating_q025": 939.2071279398446
        },
        "nvila-internal-15b-v1": {
            "rating": 957.1839963357412,
            "rating_q975": 982.6334355617619,
            "rating_q025": 931.7345571097205
        },
        "claude-3-haiku-20240307": {
            "rating": 947.2140633745418,
            "rating_q975": 960.6866066537161,
            "rating_q025": 933.7415200953674
        },
        "minicpm-v-2_6": {
            "rating": 947.1079931398152,
            "rating_q975": 965.5867984750753,
            "rating_q025": 928.6291878045552
        },
        "phi-3.5-vision-instruct": {
            "rating": 873.4751871575553,
            "rating_q975": 891.7079860383143,
            "rating_q025": 855.2423882767963
        },
        "phi-3-vision-128k-instruct": {
            "rating": 835.5419683360801,
            "rating_q975": 857.7850233112529,
            "rating_q025": 813.2989133609074
        }
    },
    "entity_recognition": {
        "grok-4-0709": {
            "rating": 1303.8448454423328,
            "rating_q975": 1334.9097861150633,
            "rating_q025": 1272.7799047696024
        },
        "gemini-2.5-pro": {
            "rating": 1279.5846717417066,
            "rating_q975": 1300.3308595644573,
            "rating_q025": 1258.838483918956
        },
        "gpt-5-high": {
            "rating": 1254.9624952684126,
            "rating_q975": 1282.6871025384394,
            "rating_q025": 1227.2378879983858
        },
        "gemini-2.5-flash": {
            "rating": 1244.834453179245,
            "rating_q975": 1270.9801607135466,
            "rating_q025": 1218.6887456449433
        },
        "o3-2025-04-16": {
            "rating": 1235.8885154062518,
            "rating_q975": 1260.533594246217,
            "rating_q025": 1211.2434365662866
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1230.203329061253,
            "rating_q975": 1265.0457165773003,
            "rating_q025": 1195.3609415452058
        },
        "gpt-5-mini-high": {
            "rating": 1220.5870443288875,
            "rating_q975": 1250.724537568708,
            "rating_q025": 1190.449551089067
        },
        "gpt-5-chat": {
            "rating": 1219.5903550608841,
            "rating_q975": 1247.7924951971902,
            "rating_q025": 1191.388214924578
        },
        "o4-mini-2025-04-16": {
            "rating": 1212.5211129323754,
            "rating_q975": 1237.8812216541194,
            "rating_q025": 1187.1610042106315
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1204.734210010853,
            "rating_q975": 1231.4929257899523,
            "rating_q025": 1177.9754942317538
        },
        "gemma-3-27b-it": {
            "rating": 1193.2702672149376,
            "rating_q975": 1221.6696706714836,
            "rating_q025": 1164.8708637583916
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1213.9512683187102,
            "rating_q025": 1162.7487316812897
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1179.2150311411372,
            "rating_q975": 1205.8298522820737,
            "rating_q025": 1152.6002100002006
        },
        "mistral-medium-2508": {
            "rating": 1164.8213892313863,
            "rating_q975": 1195.3933271628025,
            "rating_q025": 1134.2494512999701
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1143.6823675758685,
            "rating_q975": 1178.8206000281114,
            "rating_q025": 1108.5441351236257
        },
        "mistral-medium-2505": {
            "rating": 1141.0830076156717,
            "rating_q975": 1175.8175717026634,
            "rating_q025": 1106.34844352868
        },
        "mistral-small-2506": {
            "rating": 1131.290282004907,
            "rating_q975": 1168.5273134261151,
            "rating_q025": 1094.053250583699
        }
    },
    "full": {
        "gemini-2.5-pro": {
            "rating": 1262.0151484378423,
            "rating_q975": 1269.5228637614607,
            "rating_q025": 1254.5074331142239
        },
        "gemini-2.5-flash-preview-09-2025": {
            "rating": 1248.2406847308114,
            "rating_q975": 1260.1978659820177,
            "rating_q025": 1236.283503479605
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1244.3875637802885,
            "rating_q975": 1251.5273976776007,
            "rating_q025": 1237.2477298829763
        },
        "qwen3-vl-235b-a22b-instruct": {
            "rating": 1238.4000623937304,
            "rating_q975": 1250.0219049935415,
            "rating_q025": 1226.7782197939193
        },
        "gemini-2.5-flash": {
            "rating": 1233.173118571401,
            "rating_q975": 1240.7152389030289,
            "rating_q025": 1225.630998239773
        },
        "gpt-5-chat": {
            "rating": 1231.3426990965008,
            "rating_q975": 1239.7891530548034,
            "rating_q025": 1222.8962451381983
        },
        "hunyuan-vision-1.5-thinking": {
            "rating": 1218.0896827415747,
            "rating_q975": 1230.2088160246983,
            "rating_q025": 1205.9705494584512
        },
        "o3-2025-04-16": {
            "rating": 1216.6952193779791,
            "rating_q975": 1224.2595731732238,
            "rating_q025": 1209.1308655827345
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1212.6318000879035,
            "rating_q975": 1220.0280803514622,
            "rating_q025": 1205.2355198243447
        },
        "qwen-vl-max-2025-08-13": {
            "rating": 1212.3633358718648,
            "rating_q975": 1224.6076888341513,
            "rating_q025": 1200.1189829095783
        },
        "gpt-5-high": {
            "rating": 1210.7374448479381,
            "rating_q975": 1219.17012431901,
            "rating_q025": 1202.3047653768663
        },
        "qwen3-vl-235b-a22b-thinking": {
            "rating": 1209.7288071150579,
            "rating_q975": 1222.1847399512026,
            "rating_q025": 1197.2728742789132
        },
        "grok-4-0709": {
            "rating": 1207.9883540185158,
            "rating_q975": 1216.3822591197404,
            "rating_q025": 1199.5944489172912
        },
        "gemini-2.5-flash-lite-preview-09-2025-no-thinking": {
            "rating": 1204.8360856243548,
            "rating_q975": 1216.5124777559172,
            "rating_q025": 1193.1596934927925
        },
        "gpt-5-mini-high": {
            "rating": 1202.4272723482813,
            "rating_q975": 1211.1240607317884,
            "rating_q025": 1193.7304839647743
        },
        "gpt-4.5-preview-2025-02-27": {
            "rating": 1195.7220998052528,
            "rating_q975": 1207.0581137895429,
            "rating_q025": 1184.3860858209628
        },
        "claude-opus-4-20250514-thinking-16k": {
            "rating": 1195.3162737539724,
            "rating_q975": 1210.7039345925825,
            "rating_q025": 1179.9286129153622
        },
        "o4-mini-2025-04-16": {
            "rating": 1194.5535155124076,
            "rating_q975": 1202.3949711197197,
            "rating_q025": 1186.7120599050954
        },
        "claude-sonnet-4-20250514-thinking-32k": {
            "rating": 1192.1449172272935,
            "rating_q975": 1207.9973235315247,
            "rating_q025": 1176.2925109230623
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1196.4968249174976,
            "rating_q025": 1180.2031750825022
        },
        "step-1o-turbo-202506": {
            "rating": 1186.4482465885199,
            "rating_q975": 1200.6011869896638,
            "rating_q025": 1172.295306187376
        },
        "hunyuan-large-vision": {
            "rating": 1184.6527532022606,
            "rating_q975": 1200.4642093850177,
            "rating_q025": 1168.8412970195034
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1182.84038704038,
            "rating_q975": 1190.844908506347,
            "rating_q025": 1174.8358655744128
        },
        "claude-opus-4-20250514": {
            "rating": 1177.0773412818066,
            "rating_q975": 1189.544901574183,
            "rating_q025": 1164.6097809894302
        },
        "step-3": {
            "rating": 1175.7361900442345,
            "rating_q975": 1187.5354604735771,
            "rating_q025": 1163.9369196148918
        },
        "claude-sonnet-4-20250514": {
            "rating": 1175.146569534613,
            "rating_q975": 1188.5671018360497,
            "rating_q025": 1161.7260372331764
        },
        "mistral-medium-2508": {
            "rating": 1171.0056250492444,
            "rating_q975": 1179.736393839337,
            "rating_q025": 1162.2748562591516
        },
        "claude-3-7-sonnet-20250219-thinking-32k": {
            "rating": 1170.260470033673,
            "rating_q975": 1185.0669725148093,
            "rating_q025": 1155.4539675525368
        },
        "o1-2024-12-17": {
            "rating": 1169.2112776999297,
            "rating_q975": 1179.5396944959414,
            "rating_q025": 1158.882860903918
        },
        "gemma-3-27b-it": {
            "rating": 1165.0966946527526,
            "rating_q975": 1173.756532978981,
            "rating_q025": 1156.436856326524
        },
        "gemini-1.5-pro-002": {
            "rating": 1162.2208274560746,
            "rating_q975": 1170.2697555692025,
            "rating_q025": 1154.1718993429467
        },
        "gpt-5-nano-high": {
            "rating": 1160.3055006885393,
            "rating_q975": 1171.6876714680632,
            "rating_q025": 1148.9233299090154
        },
        "mistral-medium-2505": {
            "rating": 1159.2317089594621,
            "rating_q975": 1167.9197685260397,
            "rating_q025": 1150.5436493928846
        },
        "glm-4.5v": {
            "rating": 1157.9214327297918,
            "rating_q975": 1169.7484853772783,
            "rating_q025": 1146.0943800823054
        },
        "qwen2.5-vl-32b-instruct": {
            "rating": 1153.093911491737,
            "rating_q975": 1168.3880124118712,
            "rating_q025": 1137.7998105716026
        },
        "llama-4-maverick-17b-128e-instruct": {
            "rating": 1143.5361153648578,
            "rating_q975": 1152.999350018519,
            "rating_q025": 1134.0728807111966
        },
        "mistral-small-2506": {
            "rating": 1142.4103757111498,
            "rating_q975": 1152.0261296110762,
            "rating_q025": 1132.7946218112234
        },
        "gemini-1.5-flash-002": {
            "rating": 1141.2163787198965,
            "rating_q975": 1150.1914805119545,
            "rating_q025": 1132.2412769278385
        },
        "gpt-4o-2024-05-13": {
            "rating": 1137.5218812961557,
            "rating_q975": 1145.1541791353698,
            "rating_q025": 1129.8895834569416
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1135.603807241249,
            "rating_q975": 1144.5788617458197,
            "rating_q025": 1126.628752736678
        },
        "claude-3-5-sonnet-20241022": {
            "rating": 1126.1330579462165,
            "rating_q975": 1133.126910822931,
            "rating_q025": 1119.139205069502
        },
        "step-1o-vision-32k-highres": {
            "rating": 1122.7343611342699,
            "rating_q975": 1134.7596029231397,
            "rating_q025": 1110.7091193454
        },
        "claude-3-5-sonnet-20240620": {
            "rating": 1121.3816513955428,
            "rating_q975": 1130.4718052811445,
            "rating_q025": 1112.291497509941
        },
        "llama-4-scout-17b-16e-instruct": {
            "rating": 1118.3523055415822,
            "rating_q975": 1128.173677292878,
            "rating_q025": 1108.5309337902863
        },
        "qwen2.5-vl-72b-instruct": {
            "rating": 1108.4198125079063,
            "rating_q975": 1118.7428034596942,
            "rating_q025": 1098.0968215561184
        },
        "gemini-2.0-flash-lite-preview-02-05": {
            "rating": 1101.3175774541796,
            "rating_q975": 1111.2232600557106,
            "rating_q025": 1091.4118948526486
        },
        "claude-3-5-haiku-20241022": {
            "rating": 1100.2216733286627,
            "rating_q975": 1115.5824594869084,
            "rating_q025": 1084.860887170417
        },
        "gpt-4-turbo-2024-04-09": {
            "rating": 1090.8618808991585,
            "rating_q975": 1101.6470865647025,
            "rating_q025": 1080.0766752336144
        },
        "pixtral-large-2411": {
            "rating": 1090.449212444263,
            "rating_q975": 1098.9142588798989,
            "rating_q025": 1081.984166008627
        },
        "gemini-1.5-pro-001": {
            "rating": 1089.3958730752217,
            "rating_q975": 1100.2440955516533,
            "rating_q025": 1078.5476505987901
        },
        "gpt-4o-mini-2024-07-18": {
            "rating": 1066.921935969758,
            "rating_q975": 1074.1341363594527,
            "rating_q025": 1059.7097355800631
        },
        "gpt-4o-2024-08-06": {
            "rating": 1065.628195788006,
            "rating_q975": 1077.3855642951296,
            "rating_q025": 1053.8708272808824
        },
        "gpt-4.1-nano-2025-04-14": {
            "rating": 1064.1517534284144,
            "rating_q975": 1082.451666850836,
            "rating_q025": 1045.8518400059927
        },
        "qwen-vl-max-1119": {
            "rating": 1058.8702424962507,
            "rating_q975": 1074.5259283862313,
            "rating_q025": 1043.2145566062702
        },
        "qwen2-vl-72b": {
            "rating": 1047.6894320710257,
            "rating_q975": 1056.975453798212,
            "rating_q025": 1038.4034103438396
        },
        "step-1v-32k": {
            "rating": 1045.4967716077651,
            "rating_q975": 1061.654094787021,
            "rating_q025": 1029.3394484285093
        },
        "gemini-1.5-flash-8b-001": {
            "rating": 1045.0339215665736,
            "rating_q975": 1054.7669745330588,
            "rating_q025": 1035.3008686000883
        },
        "claude-3-opus-20240229": {
            "rating": 1023.848643016458,
            "rating_q975": 1033.2639553881852,
            "rating_q025": 1014.4333306447309
        },
        "molmo-72b-0924": {
            "rating": 1013.3904322206928,
            "rating_q975": 1025.783339424017,
            "rating_q025": 1000.9975250173686
        },
        "pixtral-12b-2409": {
            "rating": 1009.4874320283319,
            "rating_q975": 1018.0617829176417,
            "rating_q025": 1000.9130811390221
        },
        "gemini-1.5-flash-001": {
            "rating": 1009.1026400552914,
            "rating_q975": 1020.1751775618351,
            "rating_q025": 998.0301025487478
        },
        "internvl2-26b": {
            "rating": 1005.524036593876,
            "rating_q975": 1017.6240055636785,
            "rating_q025": 993.4240676240735
        },
        "llama-3.2-vision-90b-instruct": {
            "rating": 1000.5841360426896,
            "rating_q975": 1008.8064537087508,
            "rating_q025": 992.3618183766283
        },
        "hunyuan-standard-vision-2024-12-31": {
            "rating": 999.9692382177343,
            "rating_q975": 1021.1756064136026,
            "rating_q025": 978.762870021866
        },
        "c4ai-aya-vision-32b": {
            "rating": 996.4227668852234,
            "rating_q975": 1018.2978680288493,
            "rating_q025": 974.5476657415975
        },
        "amazon-nova-lite-v1.0": {
            "rating": 991.3335314196427,
            "rating_q975": 1005.7488399423679,
            "rating_q025": 976.9182228969174
        },
        "qwen2-vl-7b-instruct": {
            "rating": 990.8703599161919,
            "rating_q975": 1000.5121110180467,
            "rating_q025": 981.2286088143371
        },
        "claude-3-sonnet-20240229": {
            "rating": 984.991006398264,
            "rating_q975": 995.6054556383255,
            "rating_q025": 974.3765571582024
        },
        "amazon-nova-pro-v1.0": {
            "rating": 981.6930289839659,
            "rating_q975": 994.9285211104501,
            "rating_q025": 968.4575368574816
        },
        "yi-vision": {
            "rating": 979.3584731766427,
            "rating_q975": 997.031052966627,
            "rating_q025": 961.6858933866584
        },
        "llama-3.2-vision-11b-instruct": {
            "rating": 967.2748446247999,
            "rating_q975": 977.7866912407832,
            "rating_q025": 956.7629980088167
        },
        "molmo-7b-d-0924": {
            "rating": 959.5188359205524,
            "rating_q975": 972.6616602246728,
            "rating_q025": 946.376011616432
        },
        "claude-3-haiku-20240307": {
            "rating": 951.1334612527454,
            "rating_q975": 962.9090136627982,
            "rating_q025": 939.3579088426926
        },
        "internvl2-4b": {
            "rating": 942.4418162329104,
            "rating_q975": 954.2221219449413,
            "rating_q025": 930.6615105208795
        },
        "nvila-internal-15b-v1": {
            "rating": 940.9664164267274,
            "rating_q975": 960.4768657219626,
            "rating_q025": 921.4559671314922
        },
        "llava-v1.6-34b": {
            "rating": 937.4940283576013,
            "rating_q975": 949.0589579063869,
            "rating_q025": 925.9290988088158
        },
        "cogvlm2-llama3-chat-19b": {
            "rating": 928.5876490922973,
            "rating_q975": 943.3101388109485,
            "rating_q025": 913.8651593736461
        },
        "llava-onevision-qwen2-72b-ov": {
            "rating": 923.9607778750881,
            "rating_q975": 941.5102385233364,
            "rating_q025": 906.4113172268397
        },
        "minicpm-v-2_6": {
            "rating": 912.6869276183374,
            "rating_q975": 928.154607551881,
            "rating_q025": 897.2192476847939
        },
        "phi-3.5-vision-instruct": {
            "rating": 852.5713027615499,
            "rating_q975": 867.8131052060849,
            "rating_q025": 837.3295003170149
        },
        "phi-3-vision-128k-instruct": {
            "rating": 812.8058558562625,
            "rating_q975": 831.1702265834411,
            "rating_q025": 794.4414851290838
        }
    },
    "homework": {
        "hunyuan-vision-1.5-thinking": {
            "rating": 1304.319038249572,
            "rating_q975": 1332.558065427798,
            "rating_q025": 1276.080011071346
        },
        "gpt-5-chat": {
            "rating": 1286.2284988643098,
            "rating_q975": 1302.8663567264368,
            "rating_q025": 1269.5906410021828
        },
        "qwen-vl-max-2025-08-13": {
            "rating": 1279.8048128637934,
            "rating_q975": 1317.111680438521,
            "rating_q025": 1242.4979452890657
        },
        "gemini-2.5-pro": {
            "rating": 1279.1913269295555,
            "rating_q975": 1291.7022921644323,
            "rating_q025": 1266.6803616946788
        },
        "gemini-2.5-flash-preview-09-2025": {
            "rating": 1274.0633337799711,
            "rating_q975": 1301.070725537894,
            "rating_q025": 1247.0559420220482
        },
        "qwen3-vl-235b-a22b-instruct": {
            "rating": 1260.134931797671,
            "rating_q975": 1285.67980473874,
            "rating_q025": 1234.5900588566021
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1247.232940583636,
            "rating_q975": 1263.2414324378,
            "rating_q025": 1231.224448729472
        },
        "gemini-2.5-flash": {
            "rating": 1241.9277801902915,
            "rating_q975": 1255.366419618262,
            "rating_q025": 1228.489140762321
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1241.183256188206,
            "rating_q975": 1255.307678020765,
            "rating_q025": 1227.0588343556472
        },
        "qwen3-vl-235b-a22b-thinking": {
            "rating": 1240.277036294444,
            "rating_q975": 1267.9639528707507,
            "rating_q025": 1212.5901197181374
        },
        "claude-opus-4-20250514-thinking-16k": {
            "rating": 1235.5701819338087,
            "rating_q975": 1275.772957789359,
            "rating_q025": 1195.3674060782585
        },
        "o3-2025-04-16": {
            "rating": 1234.5126321560365,
            "rating_q975": 1248.1779786445404,
            "rating_q025": 1220.8472856675326
        },
        "o4-mini-2025-04-16": {
            "rating": 1233.8572318810448,
            "rating_q975": 1247.8239898968036,
            "rating_q025": 1219.890473865286
        },
        "gpt-5-high": {
            "rating": 1230.8824831614957,
            "rating_q975": 1247.550659322269,
            "rating_q025": 1214.2143070007223
        },
        "gemini-2.5-flash-lite-preview-09-2025-no-thinking": {
            "rating": 1229.1836883249885,
            "rating_q975": 1254.2973701675626,
            "rating_q025": 1204.0700064824143
        },
        "claude-sonnet-4-20250514": {
            "rating": 1218.0270116135034,
            "rating_q975": 1249.237013952736,
            "rating_q025": 1186.8170092742707
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1217.9157181465853,
            "rating_q975": 1232.6171239723456,
            "rating_q025": 1203.214312320825
        },
        "claude-opus-4-20250514": {
            "rating": 1217.095743722027,
            "rating_q975": 1246.1386524488457,
            "rating_q025": 1188.0528349952083
        },
        "gpt-5-mini-high": {
            "rating": 1216.0488605899443,
            "rating_q975": 1233.6334725223069,
            "rating_q025": 1198.4642486575817
        },
        "claude-3-7-sonnet-20250219-thinking-32k": {
            "rating": 1194.7391562896023,
            "rating_q975": 1231.7439071284114,
            "rating_q025": 1157.7344054507932
        },
        "step-3": {
            "rating": 1189.3042099917154,
            "rating_q975": 1223.2172426567697,
            "rating_q025": 1155.3911773266611
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1202.7593480765622,
            "rating_q025": 1173.9406519234376
        },
        "mistral-medium-2508": {
            "rating": 1174.6276840319788,
            "rating_q975": 1192.8486450459814,
            "rating_q025": 1156.4067230179762
        },
        "mistral-small-2506": {
            "rating": 1170.7365716841427,
            "rating_q975": 1191.0384837344598,
            "rating_q025": 1150.4346596338257
        },
        "llama-4-maverick-17b-128e-instruct": {
            "rating": 1168.5165422259156,
            "rating_q975": 1189.8955847192444,
            "rating_q025": 1147.1374997325868
        },
        "gemma-3-27b-it": {
            "rating": 1166.6894579577481,
            "rating_q975": 1183.3566525084489,
            "rating_q025": 1150.0222634070474
        },
        "glm-4.5v": {
            "rating": 1166.570740627787,
            "rating_q975": 1200.9365179593788,
            "rating_q025": 1132.2049632961953
        },
        "grok-4-0709": {
            "rating": 1164.7951919390414,
            "rating_q975": 1182.438409599525,
            "rating_q025": 1147.1519742785579
        },
        "step-1o-turbo-202506": {
            "rating": 1164.5142967101228,
            "rating_q975": 1196.2656107288353,
            "rating_q025": 1132.7629826914103
        },
        "mistral-medium-2505": {
            "rating": 1163.4608375341659,
            "rating_q975": 1181.3174207210227,
            "rating_q025": 1145.604254347309
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1156.9521065478775,
            "rating_q975": 1175.0760856429986,
            "rating_q025": 1138.8281274527565
        },
        "llama-4-scout-17b-16e-instruct": {
            "rating": 1153.8093645213767,
            "rating_q975": 1176.6455096827456,
            "rating_q025": 1130.9732193600078
        },
        "gpt-5-nano-high": {
            "rating": 1150.651552687314,
            "rating_q975": 1181.6857449063068,
            "rating_q025": 1119.6173604683213
        },
        "hunyuan-large-vision": {
            "rating": 1150.2252178478473,
            "rating_q975": 1190.4576170601204,
            "rating_q025": 1109.9928186355742
        },
        "claude-3-5-sonnet-20241022": {
            "rating": 1146.1631047948424,
            "rating_q975": 1180.1345714220654,
            "rating_q025": 1112.1916381676194
        },
        "claude-3-5-haiku-20241022": {
            "rating": 1126.7986054581552,
            "rating_q975": 1160.1937747541594,
            "rating_q025": 1093.403436162151
        }
    },
    "humor": {
        "gemini-2.5-flash-preview-09-2025": {
            "rating": 1267.7373878899841,
            "rating_q975": 1313.556591585903,
            "rating_q025": 1221.9181841940651
        },
        "gemini-2.5-pro": {
            "rating": 1250.2603690280366,
            "rating_q975": 1264.7655791912111,
            "rating_q025": 1235.755158864862
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1244.2772607821555,
            "rating_q975": 1270.1082853677185,
            "rating_q025": 1218.4462361965925
        },
        "gpt-5-chat": {
            "rating": 1226.097966528579,
            "rating_q975": 1243.9047038727392,
            "rating_q025": 1208.2912291844189
        },
        "o3-2025-04-16": {
            "rating": 1223.5072970250787,
            "rating_q975": 1239.1080842737415,
            "rating_q025": 1207.906509776416
        },
        "gemini-2.5-flash": {
            "rating": 1221.3736263140954,
            "rating_q975": 1238.0569621363034,
            "rating_q025": 1204.6902904918875
        },
        "grok-4-0709": {
            "rating": 1212.761341010934,
            "rating_q975": 1230.8883214339248,
            "rating_q025": 1194.6343605879433
        },
        "gpt-5-high": {
            "rating": 1206.8092930923854,
            "rating_q975": 1225.0538655595358,
            "rating_q025": 1188.564720625235
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1205.5972932230625,
            "rating_q975": 1222.5799991959016,
            "rating_q025": 1188.6145872502234
        },
        "qwen3-vl-235b-a22b-instruct": {
            "rating": 1201.4735983876906,
            "rating_q975": 1245.8977765258287,
            "rating_q025": 1157.0494202495524
        },
        "o4-mini-2025-04-16": {
            "rating": 1194.5720083300237,
            "rating_q975": 1211.3813620879198,
            "rating_q025": 1177.7626545721275
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1205.0067601019146,
            "rating_q025": 1171.6932398980853
        },
        "gemini-2.5-flash-lite-preview-09-2025-no-thinking": {
            "rating": 1186.0937411404443,
            "rating_q975": 1228.0237072610748,
            "rating_q025": 1144.1637750198138
        },
        "gpt-5-mini-high": {
            "rating": 1186.0522811736132,
            "rating_q975": 1205.9084994202915,
            "rating_q025": 1166.196062926935
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1179.3545123832707,
            "rating_q975": 1196.1405310273149,
            "rating_q025": 1162.5684937392266
        },
        "mistral-medium-2508": {
            "rating": 1178.538478012012,
            "rating_q975": 1197.587183972528,
            "rating_q025": 1159.4897720514962
        },
        "gemma-3-27b-it": {
            "rating": 1157.7902409230778,
            "rating_q975": 1178.7235560572701,
            "rating_q025": 1136.8569257888855
        },
        "mistral-medium-2505": {
            "rating": 1143.4170226810572,
            "rating_q975": 1170.1151676290979,
            "rating_q025": 1116.7188777330166
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1129.7065239879128,
            "rating_q975": 1150.4361254574012,
            "rating_q025": 1108.9769225184243
        },
        "mistral-small-2506": {
            "rating": 1104.0478875104623,
            "rating_q975": 1132.5448991383375,
            "rating_q025": 1075.550875882587
        },
        "llama-4-maverick-17b-128e-instruct": {
            "rating": 1093.3330902545401,
            "rating_q975": 1129.5097476544634,
            "rating_q025": 1057.156432854617
        },
        "llama-4-scout-17b-16e-instruct": {
            "rating": 1079.8613698976271,
            "rating_q975": 1118.9162866038776,
            "rating_q025": 1040.8064531913767
        }
    },
    "ocr": {
        "gemini-2.5-pro": {
            "rating": 1274.6375836826992,
            "rating_q975": 1282.0606588483156,
            "rating_q025": 1267.2145085170828
        },
        "gemini-2.5-flash-preview-09-2025": {
            "rating": 1253.9076162835127,
            "rating_q975": 1267.723960583463,
            "rating_q025": 1240.0912719835624
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1246.7268107240925,
            "rating_q975": 1255.4244565553313,
            "rating_q025": 1238.0291648928537
        },
        "gpt-5-chat": {
            "rating": 1243.8627316001298,
            "rating_q975": 1253.3541767301977,
            "rating_q025": 1234.371286470062
        },
        "qwen3-vl-235b-a22b-instruct": {
            "rating": 1239.9354677493843,
            "rating_q975": 1253.1861708663866,
            "rating_q025": 1226.684764632382
        },
        "gemini-2.5-flash": {
            "rating": 1236.474967435127,
            "rating_q975": 1244.2336301510843,
            "rating_q025": 1228.7163047191696
        },
        "qwen-vl-max-2025-08-13": {
            "rating": 1230.2608258166115,
            "rating_q975": 1247.0962140471381,
            "rating_q025": 1213.4254375860849
        },
        "o3-2025-04-16": {
            "rating": 1221.1266624179993,
            "rating_q975": 1228.9611032479797,
            "rating_q025": 1213.292221588019
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1219.1232005706831,
            "rating_q975": 1227.2289488303256,
            "rating_q025": 1211.0174523110406
        },
        "gpt-5-high": {
            "rating": 1218.8675608039925,
            "rating_q975": 1228.2786427643018,
            "rating_q025": 1209.4564788436833
        },
        "qwen3-vl-235b-a22b-thinking": {
            "rating": 1215.901413324626,
            "rating_q975": 1230.6690988096739,
            "rating_q025": 1201.1337278395781
        },
        "hunyuan-vision-1.5-thinking": {
            "rating": 1207.5475027235059,
            "rating_q975": 1222.0892047656523,
            "rating_q025": 1193.0058006813595
        },
        "claude-opus-4-20250514-thinking-16k": {
            "rating": 1206.2998589217448,
            "rating_q975": 1225.2035505724086,
            "rating_q025": 1187.396167271081
        },
        "gemini-2.5-flash-lite-preview-09-2025-no-thinking": {
            "rating": 1205.0173245140982,
            "rating_q975": 1218.411920522196,
            "rating_q025": 1191.6227285060004
        },
        "gpt-5-mini-high": {
            "rating": 1201.028209294194,
            "rating_q975": 1210.771727987051,
            "rating_q025": 1191.2846906013372
        },
        "claude-sonnet-4-20250514-thinking-32k": {
            "rating": 1199.9828669037333,
            "rating_q975": 1219.7127487878283,
            "rating_q025": 1180.2529850196383
        },
        "o4-mini-2025-04-16": {
            "rating": 1196.8823289376032,
            "rating_q975": 1204.9473266736702,
            "rating_q025": 1188.8173312015363
        },
        "grok-4-0709": {
            "rating": 1195.2768574855388,
            "rating_q975": 1204.2762257433635,
            "rating_q025": 1186.277489227714
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1189.7355750851948,
            "rating_q975": 1198.2532431801617,
            "rating_q025": 1181.217906990228
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1196.6382526720365,
            "rating_q025": 1180.0617473279633
        },
        "claude-3-7-sonnet-20250219-thinking-32k": {
            "rating": 1184.8646576702486,
            "rating_q975": 1203.0005936827747,
            "rating_q025": 1166.7287216577224
        },
        "step-1o-turbo-202506": {
            "rating": 1179.708694783295,
            "rating_q975": 1195.4074794624305,
            "rating_q025": 1164.0099101041594
        },
        "claude-opus-4-20250514": {
            "rating": 1179.1246709408183,
            "rating_q975": 1194.105967612183,
            "rating_q025": 1164.1433742694537
        },
        "mistral-medium-2508": {
            "rating": 1173.3972302942498,
            "rating_q975": 1183.4196795648847,
            "rating_q025": 1163.3747810236148
        },
        "claude-sonnet-4-20250514": {
            "rating": 1172.357245452799,
            "rating_q975": 1188.4853532743605,
            "rating_q025": 1156.2291376312373
        },
        "hunyuan-large-vision": {
            "rating": 1169.699536238695,
            "rating_q975": 1188.4672137716132,
            "rating_q025": 1150.9318587057767
        },
        "step-3": {
            "rating": 1165.733733834861,
            "rating_q975": 1181.6744546636062,
            "rating_q025": 1149.7930130061156
        },
        "gemma-3-27b-it": {
            "rating": 1156.261606605351,
            "rating_q975": 1165.4028726790118,
            "rating_q025": 1147.1203405316903
        },
        "mistral-medium-2505": {
            "rating": 1154.7456679001716,
            "rating_q975": 1164.2776802601604,
            "rating_q025": 1145.2136555401828
        },
        "glm-4.5v": {
            "rating": 1150.8885774116097,
            "rating_q975": 1166.6955860820617,
            "rating_q025": 1135.0815687411578
        },
        "gpt-5-nano-high": {
            "rating": 1146.9943185503494,
            "rating_q975": 1161.1931818790467,
            "rating_q025": 1132.795455221652
        },
        "llama-4-maverick-17b-128e-instruct": {
            "rating": 1145.8230315706032,
            "rating_q975": 1156.6555905321943,
            "rating_q025": 1134.9904726090122
        },
        "claude-3-5-sonnet-20241022": {
            "rating": 1142.7391615246695,
            "rating_q975": 1160.5137944688001,
            "rating_q025": 1124.9645285805389
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1134.4494216005705,
            "rating_q975": 1144.079379738496,
            "rating_q025": 1124.819463462645
        },
        "mistral-small-2506": {
            "rating": 1134.4354412010273,
            "rating_q975": 1144.8262961431349,
            "rating_q025": 1124.0445862589197
        },
        "llama-4-scout-17b-16e-instruct": {
            "rating": 1119.1908709371346,
            "rating_q975": 1130.723688075929,
            "rating_q025": 1107.6580537983402
        },
        "claude-3-5-haiku-20241022": {
            "rating": 1100.7045554609724,
            "rating_q975": 1119.0536763889613,
            "rating_q025": 1082.3554345329835
        }
    }
}
{
    "captioning": {
        "gemini-2.5-pro": {
            "rating": 1249.330092706233,
            "rating_q975": 1272.188728824574,
            "rating_q025": 1226.4714565878921
        },
        "gemini-2.5-flash": {
            "rating": 1219.2744760963947,
            "rating_q975": 1246.0658179959053,
            "rating_q025": 1192.483134196884
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1211.738347395223,
            "rating_q975": 1236.9359787711219,
            "rating_q025": 1186.540716019324
        },
        "gpt-5-chat": {
            "rating": 1203.2893261688055,
            "rating_q975": 1230.0480389277186,
            "rating_q025": 1176.5306134098923
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1192.682120343919,
            "rating_q975": 1230.8644230990944,
            "rating_q025": 1154.4998175887436
        },
        "gpt-5-high": {
            "rating": 1192.6399089594865,
            "rating_q975": 1224.0551356900603,
            "rating_q025": 1161.2246822289128
        },
        "o4-mini-2025-04-16": {
            "rating": 1188.7136274883728,
            "rating_q975": 1214.184156713713,
            "rating_q025": 1163.2430982630326
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1213.7423786355237,
            "rating_q025": 1162.9576213644762
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1187.9631809937944,
            "rating_q975": 1215.2572092144464,
            "rating_q025": 1160.6691527731423
        },
        "o3-2025-04-16": {
            "rating": 1173.9891971494578,
            "rating_q975": 1198.2358555898488,
            "rating_q025": 1149.7425387090668
        },
        "gpt-5-mini-high": {
            "rating": 1164.0547722341535,
            "rating_q975": 1199.6265033379552,
            "rating_q025": 1128.4830411303517
        },
        "grok-4-0709": {
            "rating": 1162.3490393431186,
            "rating_q975": 1194.7711058900406,
            "rating_q025": 1129.9269727961967
        },
        "mistral-medium-2508": {
            "rating": 1132.6191604668763,
            "rating_q975": 1162.3547508499946,
            "rating_q025": 1102.883570083758
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1121.3139232377216,
            "rating_q975": 1151.4920958263892,
            "rating_q025": 1091.135750649054
        },
        "gemma-3-27b-it": {
            "rating": 1119.5011393930545,
            "rating_q975": 1152.391963817446,
            "rating_q025": 1086.610314968663
        },
        "mistral-medium-2505": {
            "rating": 1095.6196783270575,
            "rating_q975": 1134.9378515036328,
            "rating_q025": 1056.3015051504822
        },
        "mistral-small-2506": {
            "rating": 1062.2535568826831,
            "rating_q975": 1101.1890842926755,
            "rating_q025": 1023.3180294726908
        }
    },
    "chinese": {
        "gpt-5-chat": {
            "rating": 1285.657982685187,
            "rating_q975": 1316.5809675225767,
            "rating_q025": 1254.7349978477976
        },
        "gemini-2.5-pro": {
            "rating": 1270.9424410084719,
            "rating_q975": 1299.5382784109051,
            "rating_q025": 1242.3466036060386
        },
        "o1-2024-12-17": {
            "rating": 1255.2393925391498,
            "rating_q975": 1312.2830316534814,
            "rating_q025": 1198.1957534248181
        },
        "gemini-2.5-flash": {
            "rating": 1248.532641755189,
            "rating_q975": 1278.2609456173468,
            "rating_q025": 1218.804337893031
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1244.530594108827,
            "rating_q975": 1277.4749121330747,
            "rating_q025": 1211.5862760845794
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1237.2908020431307,
            "rating_q975": 1265.2784973226157,
            "rating_q025": 1209.3031067636457
        },
        "o3-2025-04-16": {
            "rating": 1231.308203656618,
            "rating_q975": 1260.5148162094724,
            "rating_q025": 1202.1015911037634
        },
        "gpt-5-high": {
            "rating": 1230.4180434733973,
            "rating_q975": 1262.5200296684197,
            "rating_q025": 1198.316057278375
        },
        "o4-mini-2025-04-16": {
            "rating": 1217.0439765868414,
            "rating_q975": 1247.0407741592533,
            "rating_q025": 1187.0471790144295
        },
        "grok-4-0709": {
            "rating": 1214.117295158087,
            "rating_q975": 1245.1757779898253,
            "rating_q025": 1183.0588123263487
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1212.4003561051168,
            "rating_q975": 1240.7106296193742,
            "rating_q025": 1184.0900825908593
        },
        "gpt-5-mini-high": {
            "rating": 1195.2856347237935,
            "rating_q975": 1228.260800023014,
            "rating_q025": 1162.3104694245728
        },
        "gpt-4.5-preview-2025-02-27": {
            "rating": 1193.5547589765517,
            "rating_q975": 1254.244325739242,
            "rating_q025": 1132.8651922138615
        },
        "gemini-1.5-pro-002": {
            "rating": 1191.466422652818,
            "rating_q975": 1226.4516981543775,
            "rating_q025": 1156.4811471512585
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1218.9230882159432,
            "rating_q025": 1157.7769117840567
        },
        "gemma-3-27b-it": {
            "rating": 1164.4518097319294,
            "rating_q975": 1197.5110618573176,
            "rating_q025": 1131.3925576065412
        },
        "mistral-medium-2508": {
            "rating": 1163.2066775505111,
            "rating_q975": 1195.1511937260564,
            "rating_q025": 1131.262161374966
        },
        "mistral-medium-2505": {
            "rating": 1156.1959386710034,
            "rating_q975": 1190.3606680405414,
            "rating_q025": 1122.0312093014654
        },
        "claude-3-5-sonnet-20241022": {
            "rating": 1151.3418284829033,
            "rating_q975": 1181.1747548704893,
            "rating_q025": 1121.5089020953174
        },
        "gpt-4o-2024-05-13": {
            "rating": 1146.6811449104762,
            "rating_q975": 1173.7667559701456,
            "rating_q025": 1119.5955338508068
        },
        "mistral-small-2506": {
            "rating": 1141.918364680534,
            "rating_q975": 1177.1030769533522,
            "rating_q025": 1106.733652407716
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1141.5376516137867,
            "rating_q975": 1173.774374122994,
            "rating_q025": 1109.3009291045794
        },
        "llama-4-maverick-17b-128e-instruct": {
            "rating": 1131.4773089716157,
            "rating_q975": 1173.9979193138126,
            "rating_q025": 1088.9566986294187
        },
        "gemini-1.5-flash-002": {
            "rating": 1120.6771335280016,
            "rating_q975": 1156.382535904235,
            "rating_q025": 1084.9717311517682
        },
        "qwen2.5-vl-72b-instruct": {
            "rating": 1115.9179032438822,
            "rating_q975": 1168.5400666264816,
            "rating_q025": 1063.2957398612828
        },
        "claude-3-5-sonnet-20240620": {
            "rating": 1112.6743247871254,
            "rating_q975": 1140.9995986359393,
            "rating_q025": 1084.3490509383116
        },
        "llama-4-scout-17b-16e-instruct": {
            "rating": 1104.9100017113842,
            "rating_q975": 1148.5619396595002,
            "rating_q025": 1061.2580637632682
        },
        "qwen2-vl-72b": {
            "rating": 1099.6251646313167,
            "rating_q975": 1136.4397706532452,
            "rating_q025": 1062.8105586093882
        },
        "pixtral-large-2411": {
            "rating": 1088.1740196040805,
            "rating_q975": 1135.1179316554671,
            "rating_q025": 1041.2301075526939
        },
        "internvl2-26b": {
            "rating": 1070.3785908943319,
            "rating_q975": 1110.0724455711893,
            "rating_q025": 1030.6847362174744
        },
        "gpt-4o-mini-2024-07-18": {
            "rating": 1069.7391836910715,
            "rating_q975": 1097.298864384911,
            "rating_q025": 1042.179502997232
        },
        "gemini-2.0-flash-lite-preview-02-05": {
            "rating": 1065.4565290137025,
            "rating_q975": 1122.6201084432369,
            "rating_q025": 1008.292949584168
        },
        "gpt-4o-2024-08-06": {
            "rating": 1061.8191219278174,
            "rating_q975": 1106.2825073723468,
            "rating_q025": 1017.355736483288
        },
        "gemini-1.5-pro-001": {
            "rating": 1060.8932216058447,
            "rating_q975": 1090.5381867518865,
            "rating_q025": 1031.2482564598029
        },
        "gpt-4-turbo-2024-04-09": {
            "rating": 1054.7581240376712,
            "rating_q975": 1084.9058496925966,
            "rating_q025": 1024.6103983827459
        },
        "qwen2-vl-7b-instruct": {
            "rating": 1027.7359663244824,
            "rating_q975": 1065.7469854204678,
            "rating_q025": 989.7249472284972
        },
        "claude-3-opus-20240229": {
            "rating": 1018.3530695382233,
            "rating_q975": 1048.6035429393032,
            "rating_q025": 988.1025961371433
        },
        "gemini-1.5-flash-001": {
            "rating": 1016.9672052444566,
            "rating_q975": 1047.6623294498447,
            "rating_q025": 986.2720810390687
        },
        "gemini-1.5-flash-8b-001": {
            "rating": 1007.2206117179603,
            "rating_q975": 1043.8087127136657,
            "rating_q025": 970.6325107222549
        },
        "llama-3.2-vision-90b-instruct": {
            "rating": 993.9873738462665,
            "rating_q975": 1028.8166651735116,
            "rating_q025": 959.1580825190213
        },
        "molmo-72b-0924": {
            "rating": 976.0335600319552,
            "rating_q975": 1024.033339552751,
            "rating_q025": 928.0337805111593
        },
        "claude-3-sonnet-20240229": {
            "rating": 973.4618193192657,
            "rating_q975": 1004.2334695968813,
            "rating_q025": 942.6901690416502
        },
        "claude-3-haiku-20240307": {
            "rating": 966.7393692312014,
            "rating_q975": 997.5276646539942,
            "rating_q025": 935.9510738084086
        },
        "internvl2-4b": {
            "rating": 961.974466303318,
            "rating_q975": 1010.5574735156943,
            "rating_q025": 913.3914590909417
        },
        "pixtral-12b-2409": {
            "rating": 957.811463296843,
            "rating_q975": 994.4193085190311,
            "rating_q025": 921.203618074655
        },
        "molmo-7b-d-0924": {
            "rating": 945.5328359260967,
            "rating_q975": 994.589219709967,
            "rating_q025": 896.4764521422264
        },
        "llava-v1.6-34b": {
            "rating": 944.7771889940918,
            "rating_q975": 982.5996935551515,
            "rating_q025": 906.9546844330322
        },
        "llama-3.2-vision-11b-instruct": {
            "rating": 940.7506808800699,
            "rating_q975": 983.0939350775953,
            "rating_q025": 898.4074266825445
        }
    },
    "creative_writing_vision": {
        "claude-opus-4-20250514": {
            "rating": 1258.0696698643542,
            "rating_q975": 1289.0644480958135,
            "rating_q025": 1227.0748916328948
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1256.2153750962148,
            "rating_q975": 1279.1463396411905,
            "rating_q025": 1233.284410551239
        },
        "gemini-2.5-pro": {
            "rating": 1255.2758257849237,
            "rating_q975": 1270.2238730062145,
            "rating_q025": 1240.3277785636328
        },
        "gemini-2.5-flash-preview-09-2025": {
            "rating": 1250.2154352306243,
            "rating_q975": 1285.9211198209841,
            "rating_q025": 1214.5097506402644
        },
        "grok-4-0709": {
            "rating": 1235.6201126810242,
            "rating_q975": 1253.9593535119782,
            "rating_q025": 1217.2808718500703
        },
        "gpt-5-chat": {
            "rating": 1232.0661903140776,
            "rating_q975": 1249.7818149005207,
            "rating_q025": 1214.3505657276346
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1231.7755380413332,
            "rating_q975": 1248.7380443784043,
            "rating_q025": 1214.8130317042621
        },
        "gemini-2.5-flash": {
            "rating": 1230.429736172493,
            "rating_q975": 1246.975510729413,
            "rating_q025": 1213.883961615573
        },
        "qwen3-vl-235b-a22b-thinking": {
            "rating": 1215.4305750790634,
            "rating_q975": 1253.331515042828,
            "rating_q025": 1177.5296351152988
        },
        "gemini-2.5-flash-lite-preview-09-2025-no-thinking": {
            "rating": 1198.4798015364072,
            "rating_q975": 1234.346701377108,
            "rating_q025": 1162.6129016957063
        },
        "mistral-medium-2508": {
            "rating": 1197.5455704348785,
            "rating_q975": 1216.7747115356797,
            "rating_q025": 1178.3164293340774
        },
        "claude-sonnet-4-20250514": {
            "rating": 1195.9042255083991,
            "rating_q975": 1234.6403856647848,
            "rating_q025": 1157.1680653520134
        },
        "o3-2025-04-16": {
            "rating": 1195.0245665495095,
            "rating_q975": 1211.1278846019723,
            "rating_q025": 1178.9212484970467
        },
        "qwen3-vl-235b-a22b-instruct": {
            "rating": 1194.5969664026388,
            "rating_q975": 1228.3814565557286,
            "rating_q025": 1160.812476249549
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1205.8550628681812,
            "rating_q025": 1170.8449371318186
        },
        "mistral-small-2506": {
            "rating": 1185.7965943098832,
            "rating_q975": 1213.7160328342547,
            "rating_q025": 1157.8771557855116
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1184.6359839077759,
            "rating_q975": 1202.5128923808163,
            "rating_q025": 1166.7590754347355
        },
        "gpt-5-high": {
            "rating": 1180.1579077344159,
            "rating_q975": 1200.3169668355263,
            "rating_q025": 1159.9988486333054
        },
        "gemma-3-27b-it": {
            "rating": 1171.5417917817465,
            "rating_q975": 1194.2617440998717,
            "rating_q025": 1148.8218394636212
        },
        "mistral-medium-2505": {
            "rating": 1165.1239510892187,
            "rating_q975": 1189.7261842974337,
            "rating_q025": 1140.5217178810037
        },
        "o4-mini-2025-04-16": {
            "rating": 1164.8850461101551,
            "rating_q975": 1182.209110022774,
            "rating_q025": 1147.5609821975363
        },
        "qwen-vl-max-2025-08-13": {
            "rating": 1158.3164895787122,
            "rating_q975": 1201.2263958294723,
            "rating_q025": 1115.4065833279521
        },
        "gpt-5-mini-high": {
            "rating": 1155.3025718662414,
            "rating_q975": 1177.3866384059536,
            "rating_q025": 1133.218505326529
        },
        "hunyuan-vision-1.5-thinking": {
            "rating": 1144.9419176310394,
            "rating_q975": 1181.1533255717804,
            "rating_q025": 1108.7305096902983
        },
        "llama-4-scout-17b-16e-instruct": {
            "rating": 1135.2051654885809,
            "rating_q975": 1171.6338556148646,
            "rating_q025": 1098.776475362297
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1131.7443539684623,
            "rating_q975": 1153.3999791435917,
            "rating_q025": 1110.0887287933328
        },
        "llama-4-maverick-17b-128e-instruct": {
            "rating": 1126.8394984967238,
            "rating_q975": 1164.0414685884411,
            "rating_q025": 1089.6375284050064
        },
        "gpt-5-nano-high": {
            "rating": 1092.0380172867644,
            "rating_q975": 1134.0010016826661,
            "rating_q025": 1050.0750328908628
        }
    },
    "diagram": {
        "gemini-2.5-pro": {
            "rating": 1277.2069516060046,
            "rating_q975": 1288.4318446878458,
            "rating_q025": 1265.9820585241634
        },
        "gpt-5-high": {
            "rating": 1271.5038636510208,
            "rating_q975": 1287.1592964047684,
            "rating_q025": 1255.8484308972731
        },
        "gpt-5-chat": {
            "rating": 1256.8865870590835,
            "rating_q975": 1271.1694678181777,
            "rating_q025": 1242.6037062999892
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1256.5812280571981,
            "rating_q975": 1271.0124280785642,
            "rating_q025": 1242.150028035832
        },
        "qwen-vl-max-2025-08-13": {
            "rating": 1250.5632968451487,
            "rating_q975": 1281.2079926989186,
            "rating_q025": 1219.9186009913788
        },
        "o3-2025-04-16": {
            "rating": 1245.3886084918217,
            "rating_q975": 1257.7699811980879,
            "rating_q025": 1233.0072357855556
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1233.299322394807,
            "rating_q975": 1245.8400568703616,
            "rating_q025": 1220.7585879192522
        },
        "gemini-2.5-flash-preview-09-2025": {
            "rating": 1232.8266900715953,
            "rating_q975": 1256.098628116521,
            "rating_q025": 1209.5547520266696
        },
        "claude-sonnet-4-20250514-thinking-32k": {
            "rating": 1231.724661691282,
            "rating_q975": 1267.6146025727883,
            "rating_q025": 1195.834720809776
        },
        "hunyuan-vision-1.5-thinking": {
            "rating": 1226.6129131813361,
            "rating_q975": 1251.744916566202,
            "rating_q025": 1201.4809097964703
        },
        "gemini-2.5-flash": {
            "rating": 1224.0642834222178,
            "rating_q975": 1236.018264141956,
            "rating_q025": 1212.1103027024797
        },
        "qwen3-vl-235b-a22b-instruct": {
            "rating": 1223.9661686314353,
            "rating_q975": 1246.8847257463237,
            "rating_q025": 1201.0476115165468
        },
        "o4-mini-2025-04-16": {
            "rating": 1220.9730865822612,
            "rating_q975": 1233.4276159452438,
            "rating_q025": 1208.5185572192786
        },
        "claude-opus-4-20250514-thinking-16k": {
            "rating": 1219.4154238168942,
            "rating_q975": 1253.2055258366763,
            "rating_q025": 1185.625321797112
        },
        "gpt-5-mini-high": {
            "rating": 1218.9196579870006,
            "rating_q975": 1235.4088880015279,
            "rating_q025": 1202.4304279724734
        },
        "claude-3-7-sonnet-20250219-thinking-32k": {
            "rating": 1215.2859319436338,
            "rating_q975": 1247.7767156486652,
            "rating_q025": 1182.7951482386025
        },
        "qwen3-vl-235b-a22b-thinking": {
            "rating": 1211.6130965480652,
            "rating_q975": 1236.5955096431255,
            "rating_q025": 1186.6306834530048
        },
        "claude-opus-4-20250514": {
            "rating": 1209.0433166305274,
            "rating_q975": 1233.2652184582944,
            "rating_q025": 1184.8214148027605
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1205.2472361373334,
            "rating_q975": 1218.185337746512,
            "rating_q025": 1192.3091345281548
        },
        "claude-sonnet-4-20250514": {
            "rating": 1201.7163947186082,
            "rating_q975": 1229.1559876975818,
            "rating_q025": 1174.2768017396345
        },
        "grok-4-0709": {
            "rating": 1191.6563922990863,
            "rating_q975": 1205.350671126165,
            "rating_q025": 1177.9621134720076
        },
        "gemini-2.5-flash-lite-preview-09-2025-no-thinking": {
            "rating": 1188.357683937114,
            "rating_q975": 1211.551356022732,
            "rating_q025": 1165.1640118514958
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1201.1356466128002,
            "rating_q025": 1175.5643533871996
        },
        "claude-3-5-sonnet-20241022": {
            "rating": 1178.3383455049466,
            "rating_q975": 1209.7207625024994,
            "rating_q025": 1146.9559285073938
        },
        "glm-4.5v": {
            "rating": 1175.603970489372,
            "rating_q975": 1203.4635588314006,
            "rating_q025": 1147.7443821473432
        },
        "mistral-medium-2508": {
            "rating": 1168.90347828034,
            "rating_q975": 1184.0307558508034,
            "rating_q025": 1153.7762007098768
        },
        "step-1o-turbo-202506": {
            "rating": 1165.0069714753004,
            "rating_q975": 1193.3301900506283,
            "rating_q025": 1136.6837528999724
        },
        "mistral-medium-2505": {
            "rating": 1161.8161426148808,
            "rating_q975": 1176.7848864243776,
            "rating_q025": 1146.847398805384
        },
        "gemma-3-27b-it": {
            "rating": 1160.0696473285536,
            "rating_q975": 1174.7276789804355,
            "rating_q025": 1145.4116156766718
        },
        "gpt-5-nano-high": {
            "rating": 1157.2674998945508,
            "rating_q975": 1184.7149362485486,
            "rating_q025": 1129.820063540553
        },
        "llama-4-maverick-17b-128e-instruct": {
            "rating": 1150.671685960558,
            "rating_q975": 1168.8390792102355,
            "rating_q025": 1132.5042927108807
        },
        "llama-4-scout-17b-16e-instruct": {
            "rating": 1146.773333245068,
            "rating_q975": 1165.9396680732684,
            "rating_q025": 1127.6069984168676
        },
        "step-3": {
            "rating": 1139.2705173059396,
            "rating_q975": 1167.5772988686495,
            "rating_q025": 1110.9637357432298
        },
        "claude-3-5-haiku-20241022": {
            "rating": 1138.4058930478795,
            "rating_q975": 1170.3688206114632,
            "rating_q025": 1106.4429654842959
        },
        "mistral-small-2506": {
            "rating": 1137.7554575342588,
            "rating_q975": 1154.9031277649176,
            "rating_q025": 1120.6077873036
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1132.668434554027,
            "rating_q975": 1147.5054851432233,
            "rating_q025": 1117.8313839648308
        },
        "hunyuan-large-vision": {
            "rating": 1128.0558483032628,
            "rating_q975": 1161.4648683152964,
            "rating_q025": 1094.6468282912292
        }
    },
    "english": {
        "gemini-2.5-pro": {
            "rating": 1256.922085592685,
            "rating_q975": 1267.1338661181799,
            "rating_q025": 1246.7103050671903
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1237.1348390049047,
            "rating_q975": 1246.8199021805212,
            "rating_q025": 1227.4497758292882
        },
        "o3-2025-04-16": {
            "rating": 1227.4380736783012,
            "rating_q975": 1237.397989810087,
            "rating_q025": 1217.4781575465154
        },
        "gemini-2.5-flash-preview-09-2025": {
            "rating": 1226.1360151878073,
            "rating_q975": 1243.6386108477268,
            "rating_q025": 1208.6334195278878
        },
        "gpt-5-chat": {
            "rating": 1225.3855622392066,
            "rating_q975": 1237.146008177164,
            "rating_q025": 1213.6251163012491
        },
        "gemini-2.5-flash": {
            "rating": 1219.9389351754826,
            "rating_q975": 1230.3310628389129,
            "rating_q025": 1209.5468075120523
        },
        "gpt-4.5-preview-2025-02-27": {
            "rating": 1219.8299995606353,
            "rating_q975": 1235.5459397493453,
            "rating_q025": 1204.1140593719254
        },
        "gpt-5-high": {
            "rating": 1215.525478548454,
            "rating_q975": 1227.5645780369907,
            "rating_q025": 1203.4863790599172
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1215.1697639053118,
            "rating_q975": 1225.2345535182508,
            "rating_q025": 1205.1049742923728
        },
        "qwen3-vl-235b-a22b-instruct": {
            "rating": 1211.5201670120605,
            "rating_q975": 1227.9797336572137,
            "rating_q025": 1195.0606003669072
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1210.8625620578068,
            "rating_q975": 1221.3188930669514,
            "rating_q025": 1200.4062310486622
        },
        "hunyuan-vision-1.5-thinking": {
            "rating": 1208.7241974169308,
            "rating_q975": 1226.396961212276,
            "rating_q025": 1191.0514336215856
        },
        "o4-mini-2025-04-16": {
            "rating": 1203.5855323405808,
            "rating_q975": 1213.9425218330373,
            "rating_q025": 1193.2285428481243
        },
        "qwen-vl-max-2025-08-13": {
            "rating": 1201.0952571418306,
            "rating_q975": 1218.687929489866,
            "rating_q025": 1183.5025847937952
        },
        "claude-sonnet-4-20250514-thinking-32k": {
            "rating": 1198.9093904873746,
            "rating_q975": 1222.8253224714933,
            "rating_q025": 1174.9934585032559
        },
        "qwen3-vl-235b-a22b-thinking": {
            "rating": 1196.3743437168823,
            "rating_q975": 1214.6195930512865,
            "rating_q025": 1178.1290943824781
        },
        "gpt-5-mini-high": {
            "rating": 1193.371125231969,
            "rating_q975": 1205.7051901504044,
            "rating_q025": 1181.0370603135336
        },
        "claude-opus-4-20250514": {
            "rating": 1192.581057233934,
            "rating_q975": 1210.5469842773139,
            "rating_q025": 1174.615130190554
        },
        "grok-4-0709": {
            "rating": 1191.1290229488636,
            "rating_q975": 1202.6551185266906,
            "rating_q025": 1179.6029273710367
        },
        "claude-opus-4-20250514-thinking-16k": {
            "rating": 1190.465513802677,
            "rating_q975": 1214.4587582138865,
            "rating_q025": 1166.4722693914673
        },
        "gemini-2.5-flash-lite-preview-09-2025-no-thinking": {
            "rating": 1189.904172918218,
            "rating_q975": 1206.909037422297,
            "rating_q025": 1172.8993084141389
        },
        "claude-3-7-sonnet-20250219-thinking-32k": {
            "rating": 1189.187260856017,
            "rating_q975": 1211.0138800092313,
            "rating_q025": 1167.3606417028027
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1199.3002697556615,
            "rating_q025": 1177.3997302443383
        },
        "o1-2024-12-17": {
            "rating": 1187.2578996636912,
            "rating_q975": 1201.061910313064,
            "rating_q025": 1173.4538890143185
        },
        "glm-4.5v": {
            "rating": 1182.9060908146746,
            "rating_q975": 1199.7835543597953,
            "rating_q025": 1166.028627269554
        },
        "claude-sonnet-4-20250514": {
            "rating": 1176.8639103414591,
            "rating_q975": 1197.4149917590844,
            "rating_q025": 1156.3128289238339
        },
        "step-1o-turbo-202506": {
            "rating": 1175.190217304704,
            "rating_q975": 1195.2460918125398,
            "rating_q025": 1155.1343427968682
        },
        "gemini-1.5-pro-002": {
            "rating": 1170.790205154697,
            "rating_q975": 1180.8770406489964,
            "rating_q025": 1160.7033696603978
        },
        "mistral-medium-2505": {
            "rating": 1169.3861913076412,
            "rating_q975": 1180.9254999235734,
            "rating_q025": 1157.846882691709
        },
        "gpt-4o-2024-05-13": {
            "rating": 1166.6556174162738,
            "rating_q975": 1176.055744244118,
            "rating_q025": 1157.2554905884297
        },
        "gemma-3-27b-it": {
            "rating": 1163.4352289136632,
            "rating_q975": 1174.8729800380534,
            "rating_q025": 1151.997477789273
        },
        "step-3": {
            "rating": 1158.963031940188,
            "rating_q975": 1175.8417039694134,
            "rating_q025": 1142.0843599109628
        },
        "gpt-5-nano-high": {
            "rating": 1155.0733344845917,
            "rating_q975": 1171.5667488432514,
            "rating_q025": 1138.579920125932
        },
        "claude-3-5-sonnet-20241022": {
            "rating": 1153.3959985123445,
            "rating_q975": 1162.6299807685466,
            "rating_q025": 1144.1620162561424
        },
        "mistral-medium-2508": {
            "rating": 1153.0706276567485,
            "rating_q975": 1165.1630221704036,
            "rating_q025": 1140.9782331430933
        },
        "mistral-small-2506": {
            "rating": 1152.133827733603,
            "rating_q975": 1164.500955188635,
            "rating_q025": 1139.766700278571
        },
        "gemini-1.5-flash-002": {
            "rating": 1150.3462538800839,
            "rating_q975": 1161.4978138456465,
            "rating_q025": 1139.1946939145212
        },
        "hunyuan-large-vision": {
            "rating": 1150.2991897795382,
            "rating_q975": 1173.000313673916,
            "rating_q025": 1127.5980658851604
        },
        "llama-4-maverick-17b-128e-instruct": {
            "rating": 1148.681539725967,
            "rating_q975": 1161.1620048593543,
            "rating_q025": 1136.2010745925797
        },
        "gemini-2.0-flash-lite-preview-02-05": {
            "rating": 1146.612216793083,
            "rating_q975": 1159.305684910725,
            "rating_q025": 1133.918748675441
        },
        "step-1o-vision-32k-highres": {
            "rating": 1142.100128459373,
            "rating_q975": 1157.3999251586513,
            "rating_q025": 1126.8003317600949
        },
        "llama-4-scout-17b-16e-instruct": {
            "rating": 1141.34736474889,
            "rating_q975": 1154.456905805504,
            "rating_q025": 1128.237823692276
        },
        "claude-3-5-sonnet-20240620": {
            "rating": 1138.1676331607162,
            "rating_q975": 1149.6122729203596,
            "rating_q025": 1126.7229934010727
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1137.0203333453082,
            "rating_q975": 1148.579801868829,
            "rating_q025": 1125.4608648217875
        },
        "claude-3-5-haiku-20241022": {
            "rating": 1133.860808914698,
            "rating_q975": 1156.4762905884122,
            "rating_q025": 1111.245327240984
        },
        "gpt-4.1-nano-2025-04-14": {
            "rating": 1125.576019064501,
            "rating_q975": 1152.1281398479148,
            "rating_q025": 1099.0238982810872
        },
        "qwen2.5-vl-72b-instruct": {
            "rating": 1124.9177726030048,
            "rating_q975": 1137.6802207875585,
            "rating_q025": 1112.155324418451
        },
        "qwen2.5-vl-32b-instruct": {
            "rating": 1122.9628471959713,
            "rating_q975": 1142.3387549655924,
            "rating_q025": 1103.5869394263502
        },
        "gpt-4o-2024-08-06": {
            "rating": 1120.8769136006006,
            "rating_q975": 1136.0642946156945,
            "rating_q025": 1105.6895325855066
        },
        "gpt-4-turbo-2024-04-09": {
            "rating": 1113.1174667271212,
            "rating_q975": 1125.4827520604974,
            "rating_q025": 1100.7521813937449
        },
        "gemini-1.5-pro-001": {
            "rating": 1109.7797602520018,
            "rating_q975": 1121.9200592137477,
            "rating_q025": 1097.639461290256
        },
        "pixtral-large-2411": {
            "rating": 1098.270893403223,
            "rating_q975": 1109.3998347849006,
            "rating_q025": 1087.1419520215454
        },
        "gpt-4o-mini-2024-07-18": {
            "rating": 1097.6953693025084,
            "rating_q975": 1106.7480958091999,
            "rating_q025": 1088.642642795817
        },
        "qwen-vl-max-1119": {
            "rating": 1094.1585295052078,
            "rating_q975": 1114.943747134097,
            "rating_q025": 1073.3733118763184
        },
        "gemini-1.5-flash-8b-001": {
            "rating": 1086.0895644074212,
            "rating_q975": 1098.0796215362707,
            "rating_q025": 1074.0995072785718
        },
        "qwen2-vl-72b": {
            "rating": 1084.5343303357326,
            "rating_q975": 1096.2908823599575,
            "rating_q025": 1072.7777783115077
        },
        "step-1v-32k": {
            "rating": 1078.8748506428785,
            "rating_q975": 1100.137686314841,
            "rating_q025": 1057.612014970916
        },
        "molmo-72b-0924": {
            "rating": 1070.9219462559872,
            "rating_q975": 1086.8826671088889,
            "rating_q025": 1054.9612254030856
        },
        "gemini-1.5-flash-001": {
            "rating": 1069.6482976832945,
            "rating_q975": 1082.225066199599,
            "rating_q025": 1057.0715291669899
        },
        "hunyuan-standard-vision-2024-12-31": {
            "rating": 1056.692514053594,
            "rating_q975": 1083.518847228756,
            "rating_q025": 1029.8661808784323
        },
        "claude-3-opus-20240229": {
            "rating": 1054.1852533041501,
            "rating_q975": 1066.137026753546,
            "rating_q025": 1042.2334798547543
        },
        "internvl2-26b": {
            "rating": 1049.2161492897649,
            "rating_q975": 1063.3260571096926,
            "rating_q025": 1035.1062414698372
        },
        "pixtral-12b-2409": {
            "rating": 1041.2163545904662,
            "rating_q975": 1051.9973934247512,
            "rating_q025": 1030.4353157561811
        },
        "qwen2-vl-7b-instruct": {
            "rating": 1040.0895916388686,
            "rating_q975": 1052.3115413144785,
            "rating_q025": 1027.8676419632586
        },
        "llama-3.2-vision-90b-instruct": {
            "rating": 1038.399395787176,
            "rating_q975": 1048.8903959271265,
            "rating_q025": 1027.9083956472255
        },
        "molmo-7b-d-0924": {
            "rating": 1037.8678417780327,
            "rating_q975": 1054.758414949887,
            "rating_q025": 1020.9772686061784
        },
        "amazon-nova-lite-v1.0": {
            "rating": 1027.7334922427917,
            "rating_q975": 1046.6109682510353,
            "rating_q025": 1008.8560162345482
        },
        "yi-vision": {
            "rating": 1023.680630524402,
            "rating_q975": 1045.691975691705,
            "rating_q025": 1001.6692853570989
        },
        "amazon-nova-pro-v1.0": {
            "rating": 1023.4217417402515,
            "rating_q975": 1040.9721227789748,
            "rating_q025": 1005.8713607015281
        },
        "llama-3.2-vision-11b-instruct": {
            "rating": 1014.1519337109181,
            "rating_q975": 1027.417405048623,
            "rating_q025": 1000.8864623732132
        },
        "llava-onevision-qwen2-72b-ov": {
            "rating": 1009.8936694911012,
            "rating_q975": 1031.1208727343185,
            "rating_q025": 988.6664662478838
        },
        "claude-3-sonnet-20240229": {
            "rating": 1009.1539918386981,
            "rating_q975": 1021.6210004293289,
            "rating_q025": 996.6869832480672
        },
        "c4ai-aya-vision-32b": {
            "rating": 999.0056878262967,
            "rating_q975": 1028.6409014784708,
            "rating_q025": 969.3704741741226
        },
        "internvl2-4b": {
            "rating": 998.7551878591539,
            "rating_q975": 1013.6767183024067,
            "rating_q025": 983.8336574159011
        },
        "claude-3-haiku-20240307": {
            "rating": 993.1715520261564,
            "rating_q975": 1006.6433327595197,
            "rating_q025": 979.6997712927931
        },
        "nvila-internal-15b-v1": {
            "rating": 992.9050195299922,
            "rating_q975": 1018.4481540807999,
            "rating_q025": 967.3618849791844
        },
        "minicpm-v-2_6": {
            "rating": 990.8057181904999,
            "rating_q975": 1009.118465025551,
            "rating_q025": 972.4929713554487
        },
        "cogvlm2-llama3-chat-19b": {
            "rating": 990.3854584513235,
            "rating_q975": 1009.1133353614348,
            "rating_q025": 971.6575815412123
        },
        "llava-v1.6-34b": {
            "rating": 989.928876274037,
            "rating_q975": 1004.4531462441126,
            "rating_q025": 975.4046063039613
        },
        "phi-3.5-vision-instruct": {
            "rating": 936.7235579531801,
            "rating_q975": 954.9508459665012,
            "rating_q025": 918.4962699398591
        },
        "phi-3-vision-128k-instruct": {
            "rating": 900.1008316180285,
            "rating_q975": 922.2093096208819,
            "rating_q025": 877.992353615175
        }
    },
    "entity_recognition": {
        "gpt-5-high": {
            "rating": 1260.8617404515835,
            "rating_q975": 1292.7653965463396,
            "rating_q025": 1228.9580843568274
        },
        "gemini-2.5-pro": {
            "rating": 1239.0424736006287,
            "rating_q975": 1261.2970179325205,
            "rating_q025": 1216.787929268737
        },
        "grok-4-0709": {
            "rating": 1235.262496557778,
            "rating_q975": 1270.0053051844081,
            "rating_q025": 1200.5196879311477
        },
        "o3-2025-04-16": {
            "rating": 1233.5423850864158,
            "rating_q975": 1259.3116790850358,
            "rating_q025": 1207.7730910877958
        },
        "o4-mini-2025-04-16": {
            "rating": 1214.020357512472,
            "rating_q975": 1240.6409742514636,
            "rating_q025": 1187.3997407734803
        },
        "gemini-2.5-flash": {
            "rating": 1210.820488737024,
            "rating_q975": 1239.1399359987543,
            "rating_q025": 1182.501041475294
        },
        "gpt-5-mini-high": {
            "rating": 1204.4673570976056,
            "rating_q975": 1241.5548528537736,
            "rating_q025": 1167.3798613414376
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1191.4063857824049,
            "rating_q975": 1226.2743507534572,
            "rating_q025": 1156.5384208113526
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1190.1469880238776,
            "rating_q975": 1217.406578809807,
            "rating_q025": 1162.8873972379483
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1216.0475321274553,
            "rating_q025": 1160.6524678725445
        },
        "gpt-5-chat": {
            "rating": 1181.974651108717,
            "rating_q975": 1211.0523600814342,
            "rating_q025": 1152.8969421359996
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1176.9695901796345,
            "rating_q975": 1204.0809647015667,
            "rating_q025": 1149.8582156577022
        },
        "gemma-3-27b-it": {
            "rating": 1161.4657158036798,
            "rating_q975": 1190.7790782034529,
            "rating_q025": 1132.1523534039068
        },
        "mistral-medium-2508": {
            "rating": 1148.734661690121,
            "rating_q975": 1180.2419382951612,
            "rating_q025": 1117.2273850850806
        },
        "mistral-medium-2505": {
            "rating": 1122.3551236195574,
            "rating_q975": 1157.597718482583,
            "rating_q025": 1087.1125287565317
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1118.00212963712,
            "rating_q975": 1153.2335345447086,
            "rating_q025": 1082.7707247295316
        },
        "mistral-small-2506": {
            "rating": 1111.3175865051237,
            "rating_q975": 1147.8120864015505,
            "rating_q025": 1074.823086608697
        }
    },
    "full": {
        "gemini-2.5-pro": {
            "rating": 1249.6104949848582,
            "rating_q975": 1257.130972331667,
            "rating_q025": 1242.0900176380494
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1240.6560376033328,
            "rating_q975": 1247.8879256831785,
            "rating_q025": 1233.4241495234871
        },
        "gpt-4.5-preview-2025-02-27": {
            "rating": 1228.419349504668,
            "rating_q975": 1239.7470372256723,
            "rating_q025": 1217.0916617836635
        },
        "gemini-2.5-flash-preview-09-2025": {
            "rating": 1224.3914286351887,
            "rating_q975": 1236.5205883071615,
            "rating_q025": 1212.262268963216
        },
        "gpt-5-chat": {
            "rating": 1223.57600306103,
            "rating_q975": 1232.0517882817921,
            "rating_q025": 1215.1002178402678
        },
        "o3-2025-04-16": {
            "rating": 1221.7411404808554,
            "rating_q975": 1229.374202150215,
            "rating_q025": 1214.1080788114957
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1216.7600045652011,
            "rating_q975": 1224.1750564874578,
            "rating_q025": 1209.3449526429445
        },
        "gemini-2.5-flash": {
            "rating": 1215.1728988414864,
            "rating_q975": 1222.779382657752,
            "rating_q025": 1207.5664150252207
        },
        "claude-opus-4-20250514-thinking-16k": {
            "rating": 1211.0303660982663,
            "rating_q975": 1226.4645979944569,
            "rating_q025": 1195.5961342020757
        },
        "gpt-5-high": {
            "rating": 1210.0244558270842,
            "rating_q975": 1218.9132752428966,
            "rating_q025": 1201.135636411272
        },
        "claude-sonnet-4-20250514-thinking-32k": {
            "rating": 1206.4010728290223,
            "rating_q975": 1222.216802974798,
            "rating_q025": 1190.5853426832466
        },
        "qwen3-vl-235b-a22b-instruct": {
            "rating": 1204.396988844537,
            "rating_q975": 1216.1855051608063,
            "rating_q025": 1192.6084725282676
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1203.536932391561,
            "rating_q975": 1211.5648724473533,
            "rating_q025": 1195.5089923357689
        },
        "o4-mini-2025-04-16": {
            "rating": 1203.4148850845427,
            "rating_q975": 1211.3364248479759,
            "rating_q025": 1195.4933453211095
        },
        "claude-3-7-sonnet-20250219-thinking-32k": {
            "rating": 1196.6052529239123,
            "rating_q975": 1211.3963803062486,
            "rating_q025": 1181.814125541576
        },
        "o1-2024-12-17": {
            "rating": 1195.1656642956616,
            "rating_q975": 1205.4861947002473,
            "rating_q025": 1184.8451338910759
        },
        "hunyuan-vision-1.5-thinking": {
            "rating": 1194.6082177332096,
            "rating_q975": 1206.9263618510915,
            "rating_q025": 1182.2900736153276
        },
        "qwen3-vl-235b-a22b-thinking": {
            "rating": 1192.0058795215527,
            "rating_q975": 1204.6023066818013,
            "rating_q025": 1179.4094523613041
        },
        "claude-opus-4-20250514": {
            "rating": 1190.5566223325275,
            "rating_q975": 1202.9872818925926,
            "rating_q025": 1178.1259627724623
        },
        "claude-sonnet-4-20250514": {
            "rating": 1188.5432883885946,
            "rating_q975": 1202.0060691537874,
            "rating_q025": 1175.080507623402
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1196.544613444454,
            "rating_q025": 1180.1553865555459
        },
        "qwen-vl-max-2025-08-13": {
            "rating": 1185.101041479001,
            "rating_q975": 1197.506425562262,
            "rating_q025": 1172.69565739574
        },
        "gpt-5-mini-high": {
            "rating": 1184.2574185881667,
            "rating_q975": 1193.508468933367,
            "rating_q025": 1175.0063682429663
        },
        "grok-4-0709": {
            "rating": 1182.9607658552422,
            "rating_q975": 1191.4692765456102,
            "rating_q025": 1174.4522551648743
        },
        "gemini-1.5-pro-002": {
            "rating": 1180.2187610295593,
            "rating_q975": 1188.299015817145,
            "rating_q025": 1172.1385062419738
        },
        "gemini-2.5-flash-lite-preview-09-2025-no-thinking": {
            "rating": 1177.855842185862,
            "rating_q975": 1189.7264984249612,
            "rating_q025": 1165.9851859467628
        },
        "gpt-4o-2024-05-13": {
            "rating": 1164.3328022859328,
            "rating_q975": 1171.9486332065876,
            "rating_q025": 1156.716971365278
        },
        "claude-3-5-sonnet-20241022": {
            "rating": 1163.46624811116,
            "rating_q975": 1170.6065661158163,
            "rating_q025": 1156.3259301065036
        },
        "glm-4.5v": {
            "rating": 1157.9670845440567,
            "rating_q975": 1169.9030295129835,
            "rating_q025": 1146.0311395751298
        },
        "mistral-medium-2505": {
            "rating": 1157.6400532431246,
            "rating_q975": 1166.3440525898322,
            "rating_q025": 1148.936053896417
        },
        "step-1o-turbo-202506": {
            "rating": 1157.1228951019339,
            "rating_q975": 1171.53094728202,
            "rating_q025": 1142.7148429218478
        },
        "gemma-3-27b-it": {
            "rating": 1155.4615555358823,
            "rating_q975": 1164.2247328660928,
            "rating_q025": 1146.6983782056718
        },
        "mistral-medium-2508": {
            "rating": 1150.0189561204552,
            "rating_q975": 1158.8859963812872,
            "rating_q025": 1141.1519158596232
        },
        "hunyuan-large-vision": {
            "rating": 1149.8432407242403,
            "rating_q975": 1165.8661641248955,
            "rating_q025": 1133.8203173235852
        },
        "claude-3-5-sonnet-20240620": {
            "rating": 1149.3607089515622,
            "rating_q975": 1158.514384797027,
            "rating_q025": 1140.2070331060972
        },
        "gpt-5-nano-high": {
            "rating": 1147.5787036793467,
            "rating_q975": 1159.4914165479745,
            "rating_q025": 1135.665990810719
        },
        "llama-4-maverick-17b-128e-instruct": {
            "rating": 1147.3020680311888,
            "rating_q975": 1156.8167434821958,
            "rating_q025": 1137.7873925801819
        },
        "step-3": {
            "rating": 1141.921100602975,
            "rating_q975": 1154.0054429018633,
            "rating_q025": 1129.8367583040867
        },
        "gemini-1.5-flash-002": {
            "rating": 1140.626769921514,
            "rating_q975": 1149.6111111273408,
            "rating_q025": 1131.6424287156874
        },
        "mistral-small-2506": {
            "rating": 1140.294339316996,
            "rating_q975": 1149.9513573958482,
            "rating_q025": 1130.6373212381438
        },
        "claude-3-5-haiku-20241022": {
            "rating": 1136.511249174424,
            "rating_q975": 1151.8848833861655,
            "rating_q025": 1121.1376149626824
        },
        "gemini-2.0-flash-lite-preview-02-05": {
            "rating": 1135.3736718599876,
            "rating_q975": 1145.2497392673583,
            "rating_q025": 1125.4976044526168
        },
        "llama-4-scout-17b-16e-instruct": {
            "rating": 1126.9515781926475,
            "rating_q975": 1136.8253039945255,
            "rating_q025": 1117.0778523907695
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1126.8590543846835,
            "rating_q975": 1135.857070842829,
            "rating_q025": 1117.861037926538
        },
        "step-1o-vision-32k-highres": {
            "rating": 1126.3469553632322,
            "rating_q975": 1138.3947659114492,
            "rating_q025": 1114.299144815015
        },
        "qwen2.5-vl-72b-instruct": {
            "rating": 1124.236084902974,
            "rating_q975": 1134.524550675465,
            "rating_q025": 1113.947619130483
        },
        "gpt-4o-2024-08-06": {
            "rating": 1120.9398024027719,
            "rating_q975": 1132.5079017284809,
            "rating_q025": 1109.3717030770629
        },
        "gemini-1.5-pro-001": {
            "rating": 1119.7028162633171,
            "rating_q975": 1130.5003150171567,
            "rating_q025": 1108.9053175094775
        },
        "qwen2.5-vl-32b-instruct": {
            "rating": 1119.3376144152708,
            "rating_q975": 1134.8526905736223,
            "rating_q025": 1103.8225382569192
        },
        "gpt-4-turbo-2024-04-09": {
            "rating": 1115.128038710919,
            "rating_q975": 1125.8995922720503,
            "rating_q025": 1104.3564851497879
        },
        "gpt-4o-mini-2024-07-18": {
            "rating": 1099.9618985109441,
            "rating_q975": 1107.1747436790306,
            "rating_q025": 1092.7490533428577
        },
        "pixtral-large-2411": {
            "rating": 1096.2054973176982,
            "rating_q975": 1104.7089893797004,
            "rating_q025": 1087.7020052556961
        },
        "gpt-4.1-nano-2025-04-14": {
            "rating": 1091.1907393131,
            "rating_q975": 1109.4943694180445,
            "rating_q025": 1072.8871092081556
        },
        "qwen2-vl-72b": {
            "rating": 1087.6030710461184,
            "rating_q975": 1096.8568425671378,
            "rating_q025": 1078.349299525099
        },
        "qwen-vl-max-1119": {
            "rating": 1087.475681459211,
            "rating_q975": 1102.9400696399337,
            "rating_q025": 1072.0112932784884
        },
        "gemini-1.5-flash-8b-001": {
            "rating": 1072.2590166810917,
            "rating_q975": 1081.981271174783,
            "rating_q025": 1062.5367621874004
        },
        "claude-3-opus-20240229": {
            "rating": 1066.5717561906654,
            "rating_q975": 1076.0765129090578,
            "rating_q025": 1057.066999472273
        },
        "step-1v-32k": {
            "rating": 1064.1784990656452,
            "rating_q975": 1080.396079321478,
            "rating_q025": 1047.9609188098123
        },
        "gemini-1.5-flash-001": {
            "rating": 1062.185858320528,
            "rating_q975": 1073.2828709133285,
            "rating_q025": 1051.0888457277276
        },
        "molmo-72b-0924": {
            "rating": 1050.056769515979,
            "rating_q975": 1062.4727585129135,
            "rating_q025": 1037.6407805190447
        },
        "hunyuan-standard-vision-2024-12-31": {
            "rating": 1045.701209359093,
            "rating_q975": 1066.486032192584,
            "rating_q025": 1024.9163865256019
        },
        "llama-3.2-vision-90b-instruct": {
            "rating": 1034.6659847231392,
            "rating_q975": 1042.8912761605427,
            "rating_q025": 1026.4406932857357
        },
        "qwen2-vl-7b-instruct": {
            "rating": 1034.0402912535797,
            "rating_q975": 1043.7473830532313,
            "rating_q025": 1024.333199453928
        },
        "pixtral-12b-2409": {
            "rating": 1027.3904231300144,
            "rating_q975": 1035.9705442897816,
            "rating_q025": 1018.810301970247
        },
        "internvl2-26b": {
            "rating": 1026.9266249908524,
            "rating_q975": 1039.0633606248127,
            "rating_q025": 1014.7898893568919
        },
        "amazon-nova-lite-v1.0": {
            "rating": 1022.9323022931869,
            "rating_q975": 1037.4149124386597,
            "rating_q025": 1008.449692147714
        },
        "amazon-nova-pro-v1.0": {
            "rating": 1022.5130820490143,
            "rating_q975": 1035.7066535911877,
            "rating_q025": 1009.319510506841
        },
        "claude-3-sonnet-20240229": {
            "rating": 1021.7248000302261,
            "rating_q975": 1032.3343828541617,
            "rating_q025": 1011.1152172062904
        },
        "yi-vision": {
            "rating": 1006.8388961489948,
            "rating_q975": 1024.4762650814241,
            "rating_q025": 989.2015272165654
        },
        "claude-3-haiku-20240307": {
            "rating": 1004.7375023425855,
            "rating_q975": 1016.4594177208588,
            "rating_q025": 993.0155869643121
        },
        "c4ai-aya-vision-32b": {
            "rating": 1002.7146956792036,
            "rating_q975": 1024.5862702265244,
            "rating_q025": 980.8431211318828
        },
        "molmo-7b-d-0924": {
            "rating": 998.5954810538517,
            "rating_q975": 1011.7407615802113,
            "rating_q025": 985.4502005274921
        },
        "llama-3.2-vision-11b-instruct": {
            "rating": 993.4254782232732,
            "rating_q975": 1003.9184463361617,
            "rating_q025": 982.9325101103848
        },
        "nvila-internal-15b-v1": {
            "rating": 990.121064150353,
            "rating_q975": 1009.8937718053235,
            "rating_q025": 970.3483564953826
        },
        "llava-onevision-qwen2-72b-ov": {
            "rating": 983.0953908844061,
            "rating_q975": 1000.8697696242649,
            "rating_q025": 965.3210121445474
        },
        "llava-v1.6-34b": {
            "rating": 968.8058882600029,
            "rating_q975": 980.349372392827,
            "rating_q025": 957.2624041271788
        },
        "minicpm-v-2_6": {
            "rating": 967.2750464341862,
            "rating_q975": 982.5801515998809,
            "rating_q025": 951.9699412684914
        },
        "cogvlm2-llama3-chat-19b": {
            "rating": 966.5047690467608,
            "rating_q975": 981.183294216214,
            "rating_q025": 951.8262438773075
        },
        "internvl2-4b": {
            "rating": 960.1892610442948,
            "rating_q975": 972.0752715933731,
            "rating_q025": 948.3032504952164
        },
        "phi-3.5-vision-instruct": {
            "rating": 924.2937613673097,
            "rating_q975": 939.5926784791749,
            "rating_q025": 908.9948442554445
        },
        "phi-3-vision-128k-instruct": {
            "rating": 886.2111945015307,
            "rating_q975": 904.4834997390765,
            "rating_q025": 867.938889263985
        }
    },
    "homework": {
        "hunyuan-vision-1.5-thinking": {
            "rating": 1295.0173588344542,
            "rating_q975": 1323.7576269225551,
            "rating_q025": 1266.2770907463532
        },
        "gpt-5-chat": {
            "rating": 1284.5439834469908,
            "rating_q975": 1301.473159297984,
            "rating_q025": 1267.6148075959975
        },
        "gemini-2.5-pro": {
            "rating": 1270.4048011609673,
            "rating_q975": 1283.3210847796474,
            "rating_q025": 1257.4885175422871
        },
        "gpt-5-high": {
            "rating": 1267.779621169659,
            "rating_q975": 1286.010835135418,
            "rating_q025": 1249.5484072039003
        },
        "qwen-vl-max-2025-08-13": {
            "rating": 1261.2657876412493,
            "rating_q975": 1299.1811457191131,
            "rating_q025": 1223.3504295633854
        },
        "gemini-2.5-flash-preview-09-2025": {
            "rating": 1260.3310826652448,
            "rating_q975": 1287.6388047691996,
            "rating_q025": 1233.02336056129
        },
        "o4-mini-2025-04-16": {
            "rating": 1255.1538596688279,
            "rating_q975": 1269.6221805741475,
            "rating_q025": 1240.6855387635082
        },
        "o3-2025-04-16": {
            "rating": 1252.5937762114302,
            "rating_q975": 1266.858088509915,
            "rating_q025": 1238.3294639129454
        },
        "claude-opus-4-20250514-thinking-16k": {
            "rating": 1250.8040598353075,
            "rating_q975": 1291.232225352161,
            "rating_q025": 1210.375894318454
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1246.0119375889994,
            "rating_q975": 1260.551184954752,
            "rating_q025": 1231.472690223247
        },
        "gpt-5-mini-high": {
            "rating": 1245.7542861430086,
            "rating_q975": 1264.6881758550546,
            "rating_q025": 1226.8203964309625
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1241.3822878941914,
            "rating_q975": 1258.1755146077394,
            "rating_q025": 1224.5890611806435
        },
        "qwen3-vl-235b-a22b-instruct": {
            "rating": 1239.7952146098648,
            "rating_q975": 1265.8575351125667,
            "rating_q025": 1213.732894107163
        },
        "qwen3-vl-235b-a22b-thinking": {
            "rating": 1234.2967393396416,
            "rating_q975": 1262.3112131903183,
            "rating_q025": 1206.2822654889649
        },
        "gemini-2.5-flash": {
            "rating": 1232.5308600856893,
            "rating_q975": 1246.666733160464,
            "rating_q025": 1218.3949870109145
        },
        "claude-opus-4-20250514": {
            "rating": 1231.5378215991254,
            "rating_q975": 1260.6147803882552,
            "rating_q025": 1202.4608628099957
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1228.8444364506813,
            "rating_q975": 1243.707326533613,
            "rating_q025": 1213.9815463677496
        },
        "claude-sonnet-4-20250514": {
            "rating": 1228.7161408536936,
            "rating_q975": 1260.1336291515067,
            "rating_q025": 1197.2986525558804
        },
        "claude-3-7-sonnet-20250219-thinking-32k": {
            "rating": 1221.6103263440777,
            "rating_q975": 1258.421448927818,
            "rating_q025": 1184.7992037603376
        },
        "gemini-2.5-flash-lite-preview-09-2025-no-thinking": {
            "rating": 1209.3758839851225,
            "rating_q975": 1235.1228971889025,
            "rating_q025": 1183.6288707813426
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1203.1936314894579,
            "rating_q025": 1173.506368510542
        },
        "step-3": {
            "rating": 1183.4255642872886,
            "rating_q975": 1217.777386998203,
            "rating_q025": 1149.0737415763742
        },
        "claude-3-5-sonnet-20241022": {
            "rating": 1183.354667334715,
            "rating_q975": 1217.9640017038914,
            "rating_q025": 1148.7453329655386
        },
        "mistral-medium-2508": {
            "rating": 1182.097261986161,
            "rating_q975": 1200.3654025670376,
            "rating_q025": 1163.8291214052842
        },
        "gpt-5-nano-high": {
            "rating": 1181.8064895272003,
            "rating_q975": 1213.9951098193876,
            "rating_q025": 1149.617869235013
        },
        "mistral-small-2506": {
            "rating": 1177.38579461116,
            "rating_q975": 1197.6416910789399,
            "rating_q025": 1157.1298981433802
        },
        "llama-4-maverick-17b-128e-instruct": {
            "rating": 1171.885600826007,
            "rating_q975": 1193.2837701010633,
            "rating_q025": 1150.4874315509508
        },
        "gemma-3-27b-it": {
            "rating": 1170.7939409053104,
            "rating_q975": 1187.9856925470572,
            "rating_q025": 1153.6021892635636
        },
        "grok-4-0709": {
            "rating": 1170.5473158558998,
            "rating_q975": 1188.3421629373577,
            "rating_q025": 1152.752468774442
        },
        "glm-4.5v": {
            "rating": 1170.485859106891,
            "rating_q975": 1205.07430510953,
            "rating_q025": 1135.8974131042519
        },
        "mistral-medium-2505": {
            "rating": 1168.285478553875,
            "rating_q975": 1186.1678298535167,
            "rating_q025": 1150.4031272542334
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1162.467479677364,
            "rating_q975": 1180.6585145686206,
            "rating_q025": 1144.2764447861075
        },
        "claude-3-5-haiku-20241022": {
            "rating": 1160.36388716272,
            "rating_q975": 1194.2441758275463,
            "rating_q025": 1126.4835984978936
        },
        "llama-4-scout-17b-16e-instruct": {
            "rating": 1157.865746151787,
            "rating_q975": 1181.1125187786488,
            "rating_q025": 1134.6189735249254
        },
        "step-1o-turbo-202506": {
            "rating": 1144.4972759424377,
            "rating_q975": 1177.1289174547194,
            "rating_q025": 1111.865634430156
        },
        "hunyuan-large-vision": {
            "rating": 1135.8764009442812,
            "rating_q975": 1176.8140660995894,
            "rating_q025": 1094.938735788973
        }
    },
    "humor": {
        "gemini-2.5-flash-preview-09-2025": {
            "rating": 1237.4823492188161,
            "rating_q975": 1283.5961378226336,
            "rating_q025": 1191.3685606149986
        },
        "gemini-2.5-pro": {
            "rating": 1235.193643614948,
            "rating_q975": 1249.9287721537632,
            "rating_q025": 1220.458515076133
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1222.4223561171116,
            "rating_q975": 1248.5206385457761,
            "rating_q025": 1196.324073688447
        },
        "o3-2025-04-16": {
            "rating": 1220.5542744596637,
            "rating_q975": 1237.4644199999702,
            "rating_q025": 1203.6441289193572
        },
        "gpt-5-chat": {
            "rating": 1210.653355876393,
            "rating_q975": 1228.7631582632096,
            "rating_q025": 1192.5435534895767
        },
        "gpt-5-high": {
            "rating": 1209.2518114472477,
            "rating_q975": 1231.5117435690938,
            "rating_q025": 1186.9918793254017
        },
        "gemini-2.5-flash": {
            "rating": 1203.7154284961248,
            "rating_q975": 1221.0112251402018,
            "rating_q025": 1186.4196318520478
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1197.7314500102093,
            "rating_q975": 1214.9096706543125,
            "rating_q025": 1180.553229366106
        },
        "o4-mini-2025-04-16": {
            "rating": 1193.960711017839,
            "rating_q975": 1211.8910761477202,
            "rating_q025": 1176.0303458879578
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1190.4499248511274,
            "rating_q975": 1207.7951954931493,
            "rating_q025": 1173.1046542091055
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1205.4150610679792,
            "rating_q025": 1171.2849389320206
        },
        "grok-4-0709": {
            "rating": 1186.2807953548804,
            "rating_q975": 1205.0341796956475,
            "rating_q025": 1167.5274110141133
        },
        "gpt-5-mini-high": {
            "rating": 1172.590574850899,
            "rating_q975": 1196.9158873689212,
            "rating_q025": 1148.2652623328768
        },
        "qwen3-vl-235b-a22b-instruct": {
            "rating": 1157.5628124169307,
            "rating_q975": 1202.4093565780756,
            "rating_q025": 1112.7162682557857
        },
        "mistral-medium-2508": {
            "rating": 1157.2074204141609,
            "rating_q975": 1176.541886585125,
            "rating_q025": 1137.8729542431968
        },
        "gemini-2.5-flash-lite-preview-09-2025-no-thinking": {
            "rating": 1156.578984333491,
            "rating_q975": 1199.3505438515106,
            "rating_q025": 1113.8074248154714
        },
        "gemma-3-27b-it": {
            "rating": 1145.7981267857574,
            "rating_q975": 1167.4106295791441,
            "rating_q025": 1124.1856239923707
        },
        "mistral-medium-2505": {
            "rating": 1133.60969834525,
            "rating_q975": 1160.0866697995261,
            "rating_q025": 1107.132726890974
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1117.0059995645634,
            "rating_q975": 1137.769742600226,
            "rating_q025": 1096.2422565289007
        },
        "mistral-small-2506": {
            "rating": 1097.1319615444613,
            "rating_q975": 1125.3481491071393,
            "rating_q025": 1068.9157739817833
        },
        "llama-4-maverick-17b-128e-instruct": {
            "rating": 1091.3629468924755,
            "rating_q975": 1127.2629514403175,
            "rating_q025": 1055.4629423446336
        },
        "llama-4-scout-17b-16e-instruct": {
            "rating": 1080.410903449048,
            "rating_q975": 1118.9534100307867,
            "rating_q025": 1041.8683968673092
        }
    },
    "ocr": {
        "gemini-2.5-pro": {
            "rating": 1263.354987116415,
            "rating_q975": 1270.9089283953022,
            "rating_q025": 1255.801045837528
        },
        "gpt-5-chat": {
            "rating": 1241.3236493988325,
            "rating_q975": 1250.8662449756748,
            "rating_q025": 1231.78105382199
        },
        "chatgpt-4o-latest-20250326": {
            "rating": 1240.6337169389312,
            "rating_q975": 1249.6200282620857,
            "rating_q025": 1231.6474056157767
        },
        "gpt-5-high": {
            "rating": 1237.8263789341859,
            "rating_q975": 1248.2676108555538,
            "rating_q025": 1227.385147012818
        },
        "gemini-2.5-flash-preview-09-2025": {
            "rating": 1235.3553480239134,
            "rating_q975": 1249.3697524959748,
            "rating_q025": 1221.340943551852
        },
        "o3-2025-04-16": {
            "rating": 1228.6372153518537,
            "rating_q975": 1236.794126460429,
            "rating_q025": 1220.4803042432784
        },
        "claude-opus-4-20250514-thinking-16k": {
            "rating": 1224.124636587453,
            "rating_q975": 1243.0668810169707,
            "rating_q025": 1205.1823921579355
        },
        "gpt-4.1-2025-04-14": {
            "rating": 1224.0171329534617,
            "rating_q975": 1232.2623658425646,
            "rating_q025": 1215.7719000643588
        },
        "gemini-2.5-flash": {
            "rating": 1223.6649114713055,
            "rating_q975": 1231.656780624487,
            "rating_q025": 1215.6730423181239
        },
        "qwen3-vl-235b-a22b-instruct": {
            "rating": 1215.3185376735419,
            "rating_q975": 1228.7959506235738,
            "rating_q025": 1201.84112472351
        },
        "claude-sonnet-4-20250514-thinking-32k": {
            "rating": 1214.170483291603,
            "rating_q975": 1233.914633342865,
            "rating_q025": 1194.426333240341
        },
        "claude-3-7-sonnet-20250219-thinking-32k": {
            "rating": 1211.2857511270906,
            "rating_q975": 1229.4707127658814,
            "rating_q025": 1193.1007894882998
        },
        "o4-mini-2025-04-16": {
            "rating": 1210.4158378052796,
            "rating_q975": 1218.7686822793264,
            "rating_q025": 1202.0629933312327
        },
        "qwen-vl-max-2025-08-13": {
            "rating": 1208.7928934177385,
            "rating_q975": 1225.8375488977317,
            "rating_q025": 1191.7482379377452
        },
        "gpt-5-mini-high": {
            "rating": 1207.8823702778504,
            "rating_q975": 1218.7947435250333,
            "rating_q025": 1196.9699970306676
        },
        "gpt-4.1-mini-2025-04-14": {
            "rating": 1205.134235591205,
            "rating_q975": 1213.8034061382943,
            "rating_q025": 1196.4650650441156
        },
        "qwen3-vl-235b-a22b-thinking": {
            "rating": 1203.864434210403,
            "rating_q975": 1218.8385001666372,
            "rating_q025": 1188.8903682541688
        },
        "claude-opus-4-20250514": {
            "rating": 1195.1898690693033,
            "rating_q975": 1210.1655373334904,
            "rating_q025": 1180.2142008051162
        },
        "hunyuan-vision-1.5-thinking": {
            "rating": 1190.8016616808463,
            "rating_q975": 1205.5825654727678,
            "rating_q025": 1176.020757888925
        },
        "gemini-2.5-flash-lite-preview-06-17-thinking": {
            "rating": 1188.35,
            "rating_q975": 1196.788089810486,
            "rating_q025": 1179.9119101895137
        },
        "claude-sonnet-4-20250514": {
            "rating": 1185.9897861737595,
            "rating_q975": 1202.1754466611944,
            "rating_q025": 1169.8041256863246
        },
        "gemini-2.5-flash-lite-preview-09-2025-no-thinking": {
            "rating": 1184.6873848379191,
            "rating_q975": 1198.332990583081,
            "rating_q025": 1171.0417790927572
        },
        "claude-3-5-sonnet-20241022": {
            "rating": 1180.3383871534043,
            "rating_q975": 1198.4517214764364,
            "rating_q025": 1162.225052830372
        },
        "grok-4-0709": {
            "rating": 1178.3999775378998,
            "rating_q975": 1187.5392456439977,
            "rating_q025": 1169.2607094318018
        },
        "mistral-medium-2508": {
            "rating": 1167.9412161098862,
            "rating_q975": 1178.0186735074747,
            "rating_q025": 1157.8637587122978
        },
        "gpt-5-nano-high": {
            "rating": 1159.0235033676208,
            "rating_q975": 1174.1996384250742,
            "rating_q025": 1143.8473683101674
        },
        "mistral-medium-2505": {
            "rating": 1158.3835581936294,
            "rating_q975": 1167.9571861397849,
            "rating_q025": 1148.8099302474739
        },
        "gemma-3-27b-it": {
            "rating": 1157.61970368893,
            "rating_q975": 1167.046936709716,
            "rating_q025": 1148.192470668144
        },
        "glm-4.5v": {
            "rating": 1155.2874131967797,
            "rating_q975": 1171.2869036601926,
            "rating_q025": 1139.2879227333667
        },
        "step-1o-turbo-202506": {
            "rating": 1154.3079000850798,
            "rating_q975": 1170.3557772244742,
            "rating_q025": 1138.2600229456855
        },
        "llama-4-maverick-17b-128e-instruct": {
            "rating": 1152.8110348472069,
            "rating_q975": 1163.683310302885,
            "rating_q025": 1141.9387593915287
        },
        "hunyuan-large-vision": {
            "rating": 1150.2287834514746,
            "rating_q975": 1169.2980901327821,
            "rating_q025": 1131.159476770167
        },
        "step-3": {
            "rating": 1148.4532048753103,
            "rating_q975": 1164.6001781294024,
            "rating_q025": 1132.3062316212183
        },
        "mistral-small-2506": {
            "rating": 1139.1424404422303,
            "rating_q975": 1149.564315264139,
            "rating_q025": 1128.7205656203216
        },
        "claude-3-5-haiku-20241022": {
            "rating": 1139.0651004297545,
            "rating_q975": 1157.646573474759,
            "rating_q025": 1120.48362738475
        },
        "mistral-small-3.1-24b-instruct-2503": {
            "rating": 1135.6815163675165,
            "rating_q975": 1145.3158957949947,
            "rating_q025": 1126.0471369400382
        },
        "llama-4-scout-17b-16e-instruct": {
            "rating": 1129.2102561141855,
            "rating_q975": 1140.8029559726076,
            "rating_q025": 1117.6175562557635
        }
    }
}
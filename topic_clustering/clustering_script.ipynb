{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install datasets\n",
    "# %pip install bertopic\n",
    "# %pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in ./miniconda3/envs/topic_clustering/lib/python3.12/site-packages (0.26.0)\n",
      "Requirement already satisfied: filelock in ./miniconda3/envs/topic_clustering/lib/python3.12/site-packages (from huggingface_hub) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/envs/topic_clustering/lib/python3.12/site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in ./miniconda3/envs/topic_clustering/lib/python3.12/site-packages (from huggingface_hub) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/envs/topic_clustering/lib/python3.12/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in ./miniconda3/envs/topic_clustering/lib/python3.12/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./miniconda3/envs/topic_clustering/lib/python3.12/site-packages (from huggingface_hub) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/envs/topic_clustering/lib/python3.12/site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/envs/topic_clustering/lib/python3.12/site-packages (from requests->huggingface_hub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./miniconda3/envs/topic_clustering/lib/python3.12/site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/envs/topic_clustering/lib/python3.12/site-packages (from requests->huggingface_hub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/envs/topic_clustering/lib/python3.12/site-packages (from requests->huggingface_hub) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token has not been saved to git credential helper.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Connect to huggingface\n",
    "%pip install huggingface_hub\n",
    "from huggingface_hub import login\n",
    "login('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', add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"/mnt/disk0/weilin/tmp/battles_latest_20240819_freshness_20240619_md.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>conversation_a</th>\n",
       "      <th>conversation_b</th>\n",
       "      <th>turn</th>\n",
       "      <th>anony</th>\n",
       "      <th>language</th>\n",
       "      <th>tstamp</th>\n",
       "      <th>conv_metadata</th>\n",
       "      <th>is_code</th>\n",
       "      <th>is_refusal</th>\n",
       "      <th>dedup_tag</th>\n",
       "      <th>category_tag</th>\n",
       "      <th>judge_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4c6978dfa56b4ffea9d3a47e3c84181a</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>tie (bothbad)</td>\n",
       "      <td>[{'role': 'user', 'content': 'В моем портфеле ...</td>\n",
       "      <td>[{'role': 'user', 'content': 'В моем портфеле ...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Russian</td>\n",
       "      <td>1.719064e+09</td>\n",
       "      <td>{'sum_user_tokens': 290, 'sum_assistant_a_toke...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'high_freq': False, 'sampled': True}</td>\n",
       "      <td>{'if_v0.1': {'if': True, 'score': 4}, 'math_v0...</td>\n",
       "      <td>a75630e1759a83f9d476889eee3a4063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76ce56f8ba474768bc66128c7993ccb8</td>\n",
       "      <td>mistral-large-2407</td>\n",
       "      <td>athene-70b-0725</td>\n",
       "      <td>model_b</td>\n",
       "      <td>[{'role': 'user', 'content': 'php, handle tab ...</td>\n",
       "      <td>[{'role': 'user', 'content': 'php, handle tab ...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>1.722726e+09</td>\n",
       "      <td>{'sum_user_tokens': 23, 'sum_assistant_a_token...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>{'high_freq': False, 'sampled': True}</td>\n",
       "      <td>{'if_v0.1': {'if': False, 'score': 1}, 'math_v...</td>\n",
       "      <td>093c8631190fc9fed2ad75a365861d23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>385420904ba646e7a4df90c6ffae1afa</td>\n",
       "      <td>claude-3-opus-20240229</td>\n",
       "      <td>gemini-1.5-flash-api-0514</td>\n",
       "      <td>tie (bothbad)</td>\n",
       "      <td>[{'role': 'user', 'content': '普通人在愿意付出一定资源的情况下...</td>\n",
       "      <td>[{'role': 'user', 'content': '普通人在愿意付出一定资源的情况下...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>1.723119e+09</td>\n",
       "      <td>{'sum_user_tokens': 44, 'sum_assistant_a_token...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'high_freq': False, 'sampled': True}</td>\n",
       "      <td>{'if_v0.1': {'if': False, 'score': 3}, 'math_v...</td>\n",
       "      <td>a92c23ff97936574bee79f89e350ea80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e8fe7c9f75ab4e528367cc7de625c475</td>\n",
       "      <td>gemma-2-9b-it</td>\n",
       "      <td>qwen2-72b-instruct</td>\n",
       "      <td>model_b</td>\n",
       "      <td>[{'role': 'user', 'content': 'Is there any Art...</td>\n",
       "      <td>[{'role': 'user', 'content': 'Is there any Art...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>1.721643e+09</td>\n",
       "      <td>{'sum_user_tokens': 14, 'sum_assistant_a_token...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'high_freq': False, 'sampled': True}</td>\n",
       "      <td>{'if_v0.1': {'if': False, 'score': 1}, 'math_v...</td>\n",
       "      <td>26ac88d9f790142cd34c237fe369738c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>772d53e5c51c487e8a293eadcd9d4855</td>\n",
       "      <td>mixtral-8x22b-instruct-v0.1</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>tie (bothbad)</td>\n",
       "      <td>[{'role': 'user', 'content': 'Which number id ...</td>\n",
       "      <td>[{'role': 'user', 'content': 'Which number id ...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>1.721899e+09</td>\n",
       "      <td>{'sum_user_tokens': 14, 'sum_assistant_a_token...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'high_freq': False, 'sampled': True}</td>\n",
       "      <td>{'if_v0.1': {'if': False, 'score': 1}, 'math_v...</td>\n",
       "      <td>7d4cec8fb7b286fb2143cfa7b42b8eda</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id                      model_a  \\\n",
       "0  4c6978dfa56b4ffea9d3a47e3c84181a   claude-3-5-sonnet-20240620   \n",
       "1  76ce56f8ba474768bc66128c7993ccb8           mistral-large-2407   \n",
       "2  385420904ba646e7a4df90c6ffae1afa       claude-3-opus-20240229   \n",
       "3  e8fe7c9f75ab4e528367cc7de625c475                gemma-2-9b-it   \n",
       "4  772d53e5c51c487e8a293eadcd9d4855  mixtral-8x22b-instruct-v0.1   \n",
       "\n",
       "                     model_b         winner  \\\n",
       "0         gpt-3.5-turbo-0125  tie (bothbad)   \n",
       "1            athene-70b-0725        model_b   \n",
       "2  gemini-1.5-flash-api-0514  tie (bothbad)   \n",
       "3         qwen2-72b-instruct        model_b   \n",
       "4     llama-3.1-70b-instruct  tie (bothbad)   \n",
       "\n",
       "                                      conversation_a  \\\n",
       "0  [{'role': 'user', 'content': 'В моем портфеле ...   \n",
       "1  [{'role': 'user', 'content': 'php, handle tab ...   \n",
       "2  [{'role': 'user', 'content': '普通人在愿意付出一定资源的情况下...   \n",
       "3  [{'role': 'user', 'content': 'Is there any Art...   \n",
       "4  [{'role': 'user', 'content': 'Which number id ...   \n",
       "\n",
       "                                      conversation_b  turn  anony language  \\\n",
       "0  [{'role': 'user', 'content': 'В моем портфеле ...     1   True  Russian   \n",
       "1  [{'role': 'user', 'content': 'php, handle tab ...     2   True  English   \n",
       "2  [{'role': 'user', 'content': '普通人在愿意付出一定资源的情况下...     1   True  Chinese   \n",
       "3  [{'role': 'user', 'content': 'Is there any Art...     2   True  English   \n",
       "4  [{'role': 'user', 'content': 'Which number id ...     1   True  English   \n",
       "\n",
       "         tstamp                                      conv_metadata  is_code  \\\n",
       "0  1.719064e+09  {'sum_user_tokens': 290, 'sum_assistant_a_toke...    False   \n",
       "1  1.722726e+09  {'sum_user_tokens': 23, 'sum_assistant_a_token...     True   \n",
       "2  1.723119e+09  {'sum_user_tokens': 44, 'sum_assistant_a_token...    False   \n",
       "3  1.721643e+09  {'sum_user_tokens': 14, 'sum_assistant_a_token...    False   \n",
       "4  1.721899e+09  {'sum_user_tokens': 14, 'sum_assistant_a_token...    False   \n",
       "\n",
       "   is_refusal                              dedup_tag  \\\n",
       "0        True  {'high_freq': False, 'sampled': True}   \n",
       "1       False  {'high_freq': False, 'sampled': True}   \n",
       "2        True  {'high_freq': False, 'sampled': True}   \n",
       "3       False  {'high_freq': False, 'sampled': True}   \n",
       "4       False  {'high_freq': False, 'sampled': True}   \n",
       "\n",
       "                                        category_tag  \\\n",
       "0  {'if_v0.1': {'if': True, 'score': 4}, 'math_v0...   \n",
       "1  {'if_v0.1': {'if': False, 'score': 1}, 'math_v...   \n",
       "2  {'if_v0.1': {'if': False, 'score': 3}, 'math_v...   \n",
       "3  {'if_v0.1': {'if': False, 'score': 1}, 'math_v...   \n",
       "4  {'if_v0.1': {'if': False, 'score': 1}, 'math_v...   \n",
       "\n",
       "                         judge_hash  \n",
       "0  a75630e1759a83f9d476889eee3a4063  \n",
       "1  093c8631190fc9fed2ad75a365861d23  \n",
       "2  a92c23ff97936574bee79f89e350ea80  \n",
       "3  26ac88d9f790142cd34c237fe369738c  \n",
       "4  7d4cec8fb7b286fb2143cfa7b42b8eda  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60793"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_df = df[df['language'] == 'English']\n",
    "len(english_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/ygtang/topic_clustering/data/recent_english_dataset.parquet\"\n",
    "# english_df.to_parquet(file_path, index=False)\n",
    "\n",
    "english_df = load_dataset(\"parquet\", data_files=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72641"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversations_a = english_df['train']['conversation_a']\n",
    "prompts = set()\n",
    "for c in conversations_a:\n",
    "    prompts.update([interaction['content'] for interaction in c if interaction['role'] == 'user'])\n",
    "\n",
    "conversations_b = english_df['train']['conversation_b']\n",
    "for c in conversations_b:\n",
    "    prompts.update([interaction['content'] for interaction in c if interaction['role'] == 'user'])\n",
    "\n",
    "prompts = list(prompts)\n",
    "len(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d689acae05c4dcc80668f6bc2d0f32c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2271 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create embeddings\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embeddings = model.encode(prompts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72641"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/home/ygtang/topic_clustering/data/recent_english_embeddings.npy\"\n",
    "# np.save(file_path, embeddings)\n",
    "\n",
    "embeddings = np.load(file_path)\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-21 07:35:48,056 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-10-21 07:36:57,794 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-10-21 07:36:57,798 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-10-21 07:37:03,165 - BERTopic - Cluster - Completed ✓\n",
      "2024-10-21 07:37:03,177 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "2024-10-21 07:37:07,317 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "# Prepare sub-models\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=32, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "# Fit BERTopic without actually performing any clustering\n",
    "topic_model= BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        verbose=True\n",
    ").fit(prompts, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/ygtang/topic_clustering/recent_model\"\n",
    "\n",
    "# topic_model.save(\n",
    "#     path=file_path,\n",
    "#     serialization=\"safetensors\",\n",
    "#     save_ctfidf=True,\n",
    "#     save_embedding_model=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "# )\n",
    "\n",
    "topic_model = BERTopic.load(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from openai import OpenAI\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "doc_info = topic_model.get_document_info(prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = doc_info['Document']\n",
    "topics = doc_info['Topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store sampled prompts per topic\n",
    "sampled_prompts = defaultdict(list)\n",
    "\n",
    "for topic_id in topic_info['Topic']:\n",
    "    if topic_id >= 0:\n",
    "      # Get all prompts for the current topic\n",
    "      topic_prompts = [contents[i] for i in range(len(doc_info)) if topics[i] == topic_id]\n",
    "\n",
    "      s = random.sample(topic_prompts, min(10, len(topic_prompts)))\n",
    "\n",
    "      sampled_prompts[topic_id] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  The given prompts fall under the category 'Variety of miscellaneous topics including sports, error messages, creative writing, mathematics, statistical analysis, retail data, and miscellaneous questions'.\n",
      "25 :  The given prompts fall under the category 'Diverse Topics and Inquiry-Based Questions.'\n",
      "50 :  The given prompts fall under the category 'General Knowledge and Assistance'.\n",
      "75 :  The given prompts fall under the category 'Diverse General Inquiries and Creative Writing Tasks'.\n",
      "100 :  The given prompts fall under the category 'General Knowledge and Inquiry across Various Topics.'\n",
      "125 :  The given prompts fall under the category 'Miscellaneous Queries and Tasks'.\n",
      "150 :  The given prompts fall under the category 'General Knowledge and Assistance'.\n",
      "175 :  The given prompts fall under the category 'Technical and Academic Queries'.\n",
      "200 :  The given prompts fall under the category 'Mixed Topics'.\n",
      "225 :  The given prompts fall under the category 'General Knowledge and Problem-Solving'.\n"
     ]
    }
   ],
   "source": [
    "key = \"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\"\n",
    "\n",
    "def summarize_topic(prompts):\n",
    "    input_text = \"Here are some prompts for a topic:\\n\\n\" + \"\\n\\n\".join(prompts)\n",
    "    client = OpenAI(api_key=key)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Summarize the overall category of the given prompts. Respond in the format: The given prompts fall under the category '...'.\"},\n",
    "            {\"role\": \"user\", \"content\": input_text}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Summarize the prompts\n",
    "summaries = {}\n",
    "for topic_id, prompts in sampled_prompts.items():\n",
    "    summary = summarize_topic(prompts)\n",
    "    summaries[topic_id] = summary\n",
    "\n",
    "    if topic_id % 25 == 0:\n",
    "      print(topic_id, ': ', summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_df = pd.DataFrame(list(summaries.items()), columns=['Topic', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_df['Category'] = summaries_df['Summary'].apply(lambda x: re.search(r\"'(.*?)'\", x).group(1))\n",
    "topic_info_modified = topic_info[['Topic', 'Count']]\n",
    "summaries_df = summaries_df.merge(topic_info_modified, on='Topic')[['Topic', 'Category', 'Count']]\n",
    "summaries_df['Percentage'] = summaries_df['Count'] / summaries_df['Count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Category</th>\n",
       "      <th>Count</th>\n",
       "      <th>Percentage</th>\n",
       "      <th>Example Prompts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Variety of miscellaneous topics including spor...</td>\n",
       "      <td>993</td>\n",
       "      <td>0.030645</td>\n",
       "      <td>[How is Pat Cummin's personality? , CallCompac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Miscellaneous Topics</td>\n",
       "      <td>981</td>\n",
       "      <td>0.030275</td>\n",
       "      <td>[Girl in room waiting , explain in 30-40 words...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Various Technical and General Knowledge Tasks</td>\n",
       "      <td>822</td>\n",
       "      <td>0.025368</td>\n",
       "      <td>[rewite computer skills only how use technolog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Mixed Technical and General Queries</td>\n",
       "      <td>686</td>\n",
       "      <td>0.021171</td>\n",
       "      <td>[Why do some restaurants have chairs at the do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Diverse and Miscellaneous Topics</td>\n",
       "      <td>571</td>\n",
       "      <td>0.017622</td>\n",
       "      <td>[The service collects user dialogue data, incl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>243</td>\n",
       "      <td>Miscellaneous Tasks and Queries</td>\n",
       "      <td>34</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>[Translate to Persian\\n\\nThe purpose of the vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>244</td>\n",
       "      <td>technical and problem-solving queries related ...</td>\n",
       "      <td>33</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>[complete py code, 以下の英語問題ない？\\nSelect this cat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>245</td>\n",
       "      <td>Miscellaneous Topics</td>\n",
       "      <td>33</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>[why not set it to the last observed value?, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>246</td>\n",
       "      <td>Creative Writing, Math Problem Solving, and Pr...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>[\\nTell me your current version and the date o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>247</td>\n",
       "      <td>Technical and Computational Problem-Solving</td>\n",
       "      <td>32</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>[How would you pre-render text sprites at star...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>248 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Topic                                           Category  Count  \\\n",
       "0        0  Variety of miscellaneous topics including spor...    993   \n",
       "1        1                               Miscellaneous Topics    981   \n",
       "2        2      Various Technical and General Knowledge Tasks    822   \n",
       "3        3                Mixed Technical and General Queries    686   \n",
       "4        4                   Diverse and Miscellaneous Topics    571   \n",
       "..     ...                                                ...    ...   \n",
       "243    243                    Miscellaneous Tasks and Queries     34   \n",
       "244    244  technical and problem-solving queries related ...     33   \n",
       "245    245                               Miscellaneous Topics     33   \n",
       "246    246  Creative Writing, Math Problem Solving, and Pr...     32   \n",
       "247    247        Technical and Computational Problem-Solving     32   \n",
       "\n",
       "     Percentage                                    Example Prompts  \n",
       "0      0.030645  [How is Pat Cummin's personality? , CallCompac...  \n",
       "1      0.030275  [Girl in room waiting , explain in 30-40 words...  \n",
       "2      0.025368  [rewite computer skills only how use technolog...  \n",
       "3      0.021171  [Why do some restaurants have chairs at the do...  \n",
       "4      0.017622  [The service collects user dialogue data, incl...  \n",
       "..          ...                                                ...  \n",
       "243    0.001049  [Translate to Persian\\n\\nThe purpose of the vi...  \n",
       "244    0.001018  [complete py code, 以下の英語問題ない？\\nSelect this cat...  \n",
       "245    0.001018  [why not set it to the last observed value?, w...  \n",
       "246    0.000988  [\\nTell me your current version and the date o...  \n",
       "247    0.000988  [How would you pre-render text sprites at star...  \n",
       "\n",
       "[248 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/ygtang/topic_clustering/recent_english_categories_v2.csv\"\n",
    "# summaries_df.to_csv(file_path, index=False)\n",
    "summaries_df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic_clustering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

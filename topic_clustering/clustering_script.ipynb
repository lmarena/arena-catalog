{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# %pip install datasets\n",
    "# %pip install bertopic\n",
    "# %pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to huggingface\n",
    "# %pip install huggingface_hub\n",
    "# from huggingface_hub import login\n",
    "# login('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', add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"/mnt/disk0/weilin/tmp/battles_latest_20240819_freshness_20240619_md.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>conversation_a</th>\n",
       "      <th>conversation_b</th>\n",
       "      <th>turn</th>\n",
       "      <th>anony</th>\n",
       "      <th>language</th>\n",
       "      <th>tstamp</th>\n",
       "      <th>conv_metadata</th>\n",
       "      <th>is_code</th>\n",
       "      <th>is_refusal</th>\n",
       "      <th>dedup_tag</th>\n",
       "      <th>category_tag</th>\n",
       "      <th>judge_hash</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4c6978dfa56b4ffea9d3a47e3c84181a</td>\n",
       "      <td>claude-3-5-sonnet-20240620</td>\n",
       "      <td>gpt-3.5-turbo-0125</td>\n",
       "      <td>tie (bothbad)</td>\n",
       "      <td>[{'role': 'user', 'content': 'В моем портфеле ...</td>\n",
       "      <td>[{'role': 'user', 'content': 'В моем портфеле ...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Russian</td>\n",
       "      <td>1.719064e+09</td>\n",
       "      <td>{'sum_user_tokens': 290, 'sum_assistant_a_toke...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'high_freq': False, 'sampled': True}</td>\n",
       "      <td>{'if_v0.1': {'if': True, 'score': 4}, 'math_v0...</td>\n",
       "      <td>a75630e1759a83f9d476889eee3a4063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76ce56f8ba474768bc66128c7993ccb8</td>\n",
       "      <td>mistral-large-2407</td>\n",
       "      <td>athene-70b-0725</td>\n",
       "      <td>model_b</td>\n",
       "      <td>[{'role': 'user', 'content': 'php, handle tab ...</td>\n",
       "      <td>[{'role': 'user', 'content': 'php, handle tab ...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>1.722726e+09</td>\n",
       "      <td>{'sum_user_tokens': 23, 'sum_assistant_a_token...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>{'high_freq': False, 'sampled': True}</td>\n",
       "      <td>{'if_v0.1': {'if': False, 'score': 1}, 'math_v...</td>\n",
       "      <td>093c8631190fc9fed2ad75a365861d23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>385420904ba646e7a4df90c6ffae1afa</td>\n",
       "      <td>claude-3-opus-20240229</td>\n",
       "      <td>gemini-1.5-flash-api-0514</td>\n",
       "      <td>tie (bothbad)</td>\n",
       "      <td>[{'role': 'user', 'content': '普通人在愿意付出一定资源的情况下...</td>\n",
       "      <td>[{'role': 'user', 'content': '普通人在愿意付出一定资源的情况下...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>1.723119e+09</td>\n",
       "      <td>{'sum_user_tokens': 44, 'sum_assistant_a_token...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>{'high_freq': False, 'sampled': True}</td>\n",
       "      <td>{'if_v0.1': {'if': False, 'score': 3}, 'math_v...</td>\n",
       "      <td>a92c23ff97936574bee79f89e350ea80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e8fe7c9f75ab4e528367cc7de625c475</td>\n",
       "      <td>gemma-2-9b-it</td>\n",
       "      <td>qwen2-72b-instruct</td>\n",
       "      <td>model_b</td>\n",
       "      <td>[{'role': 'user', 'content': 'Is there any Art...</td>\n",
       "      <td>[{'role': 'user', 'content': 'Is there any Art...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>1.721643e+09</td>\n",
       "      <td>{'sum_user_tokens': 14, 'sum_assistant_a_token...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'high_freq': False, 'sampled': True}</td>\n",
       "      <td>{'if_v0.1': {'if': False, 'score': 1}, 'math_v...</td>\n",
       "      <td>26ac88d9f790142cd34c237fe369738c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>772d53e5c51c487e8a293eadcd9d4855</td>\n",
       "      <td>mixtral-8x22b-instruct-v0.1</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>tie (bothbad)</td>\n",
       "      <td>[{'role': 'user', 'content': 'Which number id ...</td>\n",
       "      <td>[{'role': 'user', 'content': 'Which number id ...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>1.721899e+09</td>\n",
       "      <td>{'sum_user_tokens': 14, 'sum_assistant_a_token...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'high_freq': False, 'sampled': True}</td>\n",
       "      <td>{'if_v0.1': {'if': False, 'score': 1}, 'math_v...</td>\n",
       "      <td>7d4cec8fb7b286fb2143cfa7b42b8eda</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        question_id                      model_a  \\\n",
       "0  4c6978dfa56b4ffea9d3a47e3c84181a   claude-3-5-sonnet-20240620   \n",
       "1  76ce56f8ba474768bc66128c7993ccb8           mistral-large-2407   \n",
       "2  385420904ba646e7a4df90c6ffae1afa       claude-3-opus-20240229   \n",
       "3  e8fe7c9f75ab4e528367cc7de625c475                gemma-2-9b-it   \n",
       "4  772d53e5c51c487e8a293eadcd9d4855  mixtral-8x22b-instruct-v0.1   \n",
       "\n",
       "                     model_b         winner  \\\n",
       "0         gpt-3.5-turbo-0125  tie (bothbad)   \n",
       "1            athene-70b-0725        model_b   \n",
       "2  gemini-1.5-flash-api-0514  tie (bothbad)   \n",
       "3         qwen2-72b-instruct        model_b   \n",
       "4     llama-3.1-70b-instruct  tie (bothbad)   \n",
       "\n",
       "                                      conversation_a  \\\n",
       "0  [{'role': 'user', 'content': 'В моем портфеле ...   \n",
       "1  [{'role': 'user', 'content': 'php, handle tab ...   \n",
       "2  [{'role': 'user', 'content': '普通人在愿意付出一定资源的情况下...   \n",
       "3  [{'role': 'user', 'content': 'Is there any Art...   \n",
       "4  [{'role': 'user', 'content': 'Which number id ...   \n",
       "\n",
       "                                      conversation_b  turn  anony language  \\\n",
       "0  [{'role': 'user', 'content': 'В моем портфеле ...     1   True  Russian   \n",
       "1  [{'role': 'user', 'content': 'php, handle tab ...     2   True  English   \n",
       "2  [{'role': 'user', 'content': '普通人在愿意付出一定资源的情况下...     1   True  Chinese   \n",
       "3  [{'role': 'user', 'content': 'Is there any Art...     2   True  English   \n",
       "4  [{'role': 'user', 'content': 'Which number id ...     1   True  English   \n",
       "\n",
       "         tstamp                                      conv_metadata  is_code  \\\n",
       "0  1.719064e+09  {'sum_user_tokens': 290, 'sum_assistant_a_toke...    False   \n",
       "1  1.722726e+09  {'sum_user_tokens': 23, 'sum_assistant_a_token...     True   \n",
       "2  1.723119e+09  {'sum_user_tokens': 44, 'sum_assistant_a_token...    False   \n",
       "3  1.721643e+09  {'sum_user_tokens': 14, 'sum_assistant_a_token...    False   \n",
       "4  1.721899e+09  {'sum_user_tokens': 14, 'sum_assistant_a_token...    False   \n",
       "\n",
       "   is_refusal                              dedup_tag  \\\n",
       "0        True  {'high_freq': False, 'sampled': True}   \n",
       "1       False  {'high_freq': False, 'sampled': True}   \n",
       "2        True  {'high_freq': False, 'sampled': True}   \n",
       "3       False  {'high_freq': False, 'sampled': True}   \n",
       "4       False  {'high_freq': False, 'sampled': True}   \n",
       "\n",
       "                                        category_tag  \\\n",
       "0  {'if_v0.1': {'if': True, 'score': 4}, 'math_v0...   \n",
       "1  {'if_v0.1': {'if': False, 'score': 1}, 'math_v...   \n",
       "2  {'if_v0.1': {'if': False, 'score': 3}, 'math_v...   \n",
       "3  {'if_v0.1': {'if': False, 'score': 1}, 'math_v...   \n",
       "4  {'if_v0.1': {'if': False, 'score': 1}, 'math_v...   \n",
       "\n",
       "                         judge_hash  \n",
       "0  a75630e1759a83f9d476889eee3a4063  \n",
       "1  093c8631190fc9fed2ad75a365861d23  \n",
       "2  a92c23ff97936574bee79f89e350ea80  \n",
       "3  26ac88d9f790142cd34c237fe369738c  \n",
       "4  7d4cec8fb7b286fb2143cfa7b42b8eda  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60793"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_df = df[df['language'] == 'English']\n",
    "len(english_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_df['Prompt'] = english_df.apply(lambda x: ' '.join([i['content'] for i in x['conversation_a'] if i['role'] == 'user']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>winner</th>\n",
       "      <th>conversation_a</th>\n",
       "      <th>conversation_b</th>\n",
       "      <th>turn</th>\n",
       "      <th>anony</th>\n",
       "      <th>language</th>\n",
       "      <th>tstamp</th>\n",
       "      <th>conv_metadata</th>\n",
       "      <th>is_code</th>\n",
       "      <th>is_refusal</th>\n",
       "      <th>dedup_tag</th>\n",
       "      <th>category_tag</th>\n",
       "      <th>judge_hash</th>\n",
       "      <th>Prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>76ce56f8ba474768bc66128c7993ccb8</td>\n",
       "      <td>mistral-large-2407</td>\n",
       "      <td>athene-70b-0725</td>\n",
       "      <td>model_b</td>\n",
       "      <td>[{'role': 'user', 'content': 'php, handle tab ...</td>\n",
       "      <td>[{'role': 'user', 'content': 'php, handle tab ...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>1.722726e+09</td>\n",
       "      <td>{'sum_user_tokens': 23, 'sum_assistant_a_token...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>{'high_freq': False, 'sampled': True}</td>\n",
       "      <td>{'if_v0.1': {'if': False, 'score': 1}, 'math_v...</td>\n",
       "      <td>093c8631190fc9fed2ad75a365861d23</td>\n",
       "      <td>php, handle tab in text as html, keeping them ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>e8fe7c9f75ab4e528367cc7de625c475</td>\n",
       "      <td>gemma-2-9b-it</td>\n",
       "      <td>qwen2-72b-instruct</td>\n",
       "      <td>model_b</td>\n",
       "      <td>[{'role': 'user', 'content': 'Is there any Art...</td>\n",
       "      <td>[{'role': 'user', 'content': 'Is there any Art...</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>1.721643e+09</td>\n",
       "      <td>{'sum_user_tokens': 14, 'sum_assistant_a_token...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'high_freq': False, 'sampled': True}</td>\n",
       "      <td>{'if_v0.1': {'if': False, 'score': 1}, 'math_v...</td>\n",
       "      <td>26ac88d9f790142cd34c237fe369738c</td>\n",
       "      <td>Is there any Artificial Superintelligence? Wha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>772d53e5c51c487e8a293eadcd9d4855</td>\n",
       "      <td>mixtral-8x22b-instruct-v0.1</td>\n",
       "      <td>llama-3.1-70b-instruct</td>\n",
       "      <td>tie (bothbad)</td>\n",
       "      <td>[{'role': 'user', 'content': 'Which number id ...</td>\n",
       "      <td>[{'role': 'user', 'content': 'Which number id ...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>1.721899e+09</td>\n",
       "      <td>{'sum_user_tokens': 14, 'sum_assistant_a_token...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'high_freq': False, 'sampled': True}</td>\n",
       "      <td>{'if_v0.1': {'if': False, 'score': 1}, 'math_v...</td>\n",
       "      <td>7d4cec8fb7b286fb2143cfa7b42b8eda</td>\n",
       "      <td>Which number id bigger 9.11 or 9.9 ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6ccd7a51825249d5881ee501e06bb9ab</td>\n",
       "      <td>mixtral-8x22b-instruct-v0.1</td>\n",
       "      <td>gemma-2-2b-it</td>\n",
       "      <td>model_a</td>\n",
       "      <td>[{'role': 'user', 'content': 'solve this sudok...</td>\n",
       "      <td>[{'role': 'user', 'content': 'solve this sudok...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>1.721922e+09</td>\n",
       "      <td>{'sum_user_tokens': 133, 'sum_assistant_a_toke...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>{'high_freq': False, 'sampled': True}</td>\n",
       "      <td>{'if_v0.1': {'if': True, 'score': 4}, 'math_v0...</td>\n",
       "      <td>1f71d1675fcea18e498cec67006eddeb</td>\n",
       "      <td>solve this sudoku:\\n. 2 . | 6 . . | . . .\\n. ....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>463aa4efacf34f27b6a5c3f1f7417e86</td>\n",
       "      <td>gemini-1.5-pro-api-0514</td>\n",
       "      <td>reka-flash-preview-20240611</td>\n",
       "      <td>model_a</td>\n",
       "      <td>[{'role': 'user', 'content': 'paraphrase and s...</td>\n",
       "      <td>[{'role': 'user', 'content': 'paraphrase and s...</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>English</td>\n",
       "      <td>1.719425e+09</td>\n",
       "      <td>{'sum_user_tokens': 47, 'sum_assistant_a_token...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>{'high_freq': False, 'sampled': True}</td>\n",
       "      <td>{'if_v0.1': {'if': False, 'score': 1}, 'math_v...</td>\n",
       "      <td>4e4b464f98fcea52723ebba66953fbdf</td>\n",
       "      <td>paraphrase and simplify as best you can: The s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         question_id                      model_a  \\\n",
       "1   76ce56f8ba474768bc66128c7993ccb8           mistral-large-2407   \n",
       "3   e8fe7c9f75ab4e528367cc7de625c475                gemma-2-9b-it   \n",
       "4   772d53e5c51c487e8a293eadcd9d4855  mixtral-8x22b-instruct-v0.1   \n",
       "8   6ccd7a51825249d5881ee501e06bb9ab  mixtral-8x22b-instruct-v0.1   \n",
       "11  463aa4efacf34f27b6a5c3f1f7417e86      gemini-1.5-pro-api-0514   \n",
       "\n",
       "                        model_b         winner  \\\n",
       "1               athene-70b-0725        model_b   \n",
       "3            qwen2-72b-instruct        model_b   \n",
       "4        llama-3.1-70b-instruct  tie (bothbad)   \n",
       "8                 gemma-2-2b-it        model_a   \n",
       "11  reka-flash-preview-20240611        model_a   \n",
       "\n",
       "                                       conversation_a  \\\n",
       "1   [{'role': 'user', 'content': 'php, handle tab ...   \n",
       "3   [{'role': 'user', 'content': 'Is there any Art...   \n",
       "4   [{'role': 'user', 'content': 'Which number id ...   \n",
       "8   [{'role': 'user', 'content': 'solve this sudok...   \n",
       "11  [{'role': 'user', 'content': 'paraphrase and s...   \n",
       "\n",
       "                                       conversation_b  turn  anony language  \\\n",
       "1   [{'role': 'user', 'content': 'php, handle tab ...     2   True  English   \n",
       "3   [{'role': 'user', 'content': 'Is there any Art...     2   True  English   \n",
       "4   [{'role': 'user', 'content': 'Which number id ...     1   True  English   \n",
       "8   [{'role': 'user', 'content': 'solve this sudok...     1   True  English   \n",
       "11  [{'role': 'user', 'content': 'paraphrase and s...     1   True  English   \n",
       "\n",
       "          tstamp                                      conv_metadata  is_code  \\\n",
       "1   1.722726e+09  {'sum_user_tokens': 23, 'sum_assistant_a_token...     True   \n",
       "3   1.721643e+09  {'sum_user_tokens': 14, 'sum_assistant_a_token...    False   \n",
       "4   1.721899e+09  {'sum_user_tokens': 14, 'sum_assistant_a_token...    False   \n",
       "8   1.721922e+09  {'sum_user_tokens': 133, 'sum_assistant_a_toke...     True   \n",
       "11  1.719425e+09  {'sum_user_tokens': 47, 'sum_assistant_a_token...    False   \n",
       "\n",
       "    is_refusal                              dedup_tag  \\\n",
       "1        False  {'high_freq': False, 'sampled': True}   \n",
       "3        False  {'high_freq': False, 'sampled': True}   \n",
       "4        False  {'high_freq': False, 'sampled': True}   \n",
       "8        False  {'high_freq': False, 'sampled': True}   \n",
       "11       False  {'high_freq': False, 'sampled': True}   \n",
       "\n",
       "                                         category_tag  \\\n",
       "1   {'if_v0.1': {'if': False, 'score': 1}, 'math_v...   \n",
       "3   {'if_v0.1': {'if': False, 'score': 1}, 'math_v...   \n",
       "4   {'if_v0.1': {'if': False, 'score': 1}, 'math_v...   \n",
       "8   {'if_v0.1': {'if': True, 'score': 4}, 'math_v0...   \n",
       "11  {'if_v0.1': {'if': False, 'score': 1}, 'math_v...   \n",
       "\n",
       "                          judge_hash  \\\n",
       "1   093c8631190fc9fed2ad75a365861d23   \n",
       "3   26ac88d9f790142cd34c237fe369738c   \n",
       "4   7d4cec8fb7b286fb2143cfa7b42b8eda   \n",
       "8   1f71d1675fcea18e498cec67006eddeb   \n",
       "11  4e4b464f98fcea52723ebba66953fbdf   \n",
       "\n",
       "                                               Prompt  \n",
       "1   php, handle tab in text as html, keeping them ...  \n",
       "3   Is there any Artificial Superintelligence? Wha...  \n",
       "4                Which number id bigger 9.11 or 9.9 ?  \n",
       "8   solve this sudoku:\\n. 2 . | 6 . . | . . .\\n. ....  \n",
       "11  paraphrase and simplify as best you can: The s...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "english_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/ygtang/topic_clustering/data/recent_english_dataset.parquet\"\n",
    "# english_df.to_parquet(file_path, index=False)\n",
    "\n",
    "english_df = load_dataset(\"parquet\", data_files=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60793"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc = english_df['train']['Prompt']\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec3a77e8797443a99789fbcac6173b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1900 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create embeddings\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "embeddings = model.encode(doc, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60793"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = \"/home/ygtang/topic_clustering/data/recent_english_embeddings.npy\"\n",
    "# np.save(file_path, embeddings)\n",
    "\n",
    "embeddings = np.load(file_path)\n",
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import openai\n",
    "from bertopic.representation import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-27 07:11:22,402 - BERTopic - Dimensionality - Fitting the dimensionality reduction algorithm\n",
      "2024-10-27 07:12:47,365 - BERTopic - Dimensionality - Completed ✓\n",
      "2024-10-27 07:12:47,368 - BERTopic - Cluster - Start clustering the reduced embeddings\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2024-10-27 07:12:55,254 - BERTopic - Cluster - Completed ✓\n",
      "2024-10-27 07:12:55,273 - BERTopic - Representation - Extracting topics from clusters using representation models.\n",
      "100%|██████████| 188/188 [02:37<00:00,  1.20it/s]\n",
      "2024-10-27 07:15:54,663 - BERTopic - Representation - Completed ✓\n"
     ]
    }
   ],
   "source": [
    "# Prepare sub-models\n",
    "embedding_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=40, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\", min_df=2, ngram_range=(1, 2))\n",
    "\n",
    "# OpenAi\n",
    "key = \"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\"\n",
    "prompt = \"\"\"\n",
    "I have a topic that contains the following documents:\n",
    "[DOCUMENTS]\n",
    "The topic is described by the following keywords: [KEYWORDS]\n",
    "\n",
    "Based on the information above, extract a short but highly descriptive topic label of at most 5 words. Make sure it is in the following format:\n",
    "topic: <topic label>\n",
    "\"\"\"\n",
    "client = openai.OpenAI(api_key=key)\n",
    "openai_model = OpenAI(client, model=\"gpt-4o\", exponential_backoff=True, chat=True, prompt=prompt)\n",
    "representation_model = {\"OpenAI\": openai_model}\n",
    "\n",
    "# Fit BERTopic without actually performing any clustering\n",
    "topic_model= BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        umap_model=umap_model,\n",
    "        hdbscan_model=hdbscan_model,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        representation_model=representation_model,\n",
    "        \n",
    "        top_n_words=10,\n",
    "        verbose=True\n",
    ").fit(doc, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/ygtang/topic_clustering/recent_model\"\n",
    "\n",
    "# topic_model.save(\n",
    "#     path=file_path,\n",
    "#     serialization=\"safetensors\",\n",
    "#     save_ctfidf=True,\n",
    "#     save_embedding_model=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "# )\n",
    "\n",
    "topic_model = BERTopic.load(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize Categories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from openai import OpenAI\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_info = topic_model.get_topic_info()\n",
    "doc_info = topic_model.get_document_info(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents = doc_info['Document']\n",
    "topics = doc_info['Topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>OpenAI</th>\n",
       "      <th>Representative_Docs</th>\n",
       "      <th>Top_n_words</th>\n",
       "      <th>Representative_document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>php, handle tab in text as html, keeping them ...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_self_data_return_use</td>\n",
       "      <td>[self, data, return, use, new, time, text, fun...</td>\n",
       "      <td>[{\\n    \"reasoning\": \"The last response of the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>self - data - return - use - new - time - text...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is there any Artificial Superintelligence? Wha...</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_self_data_return_use</td>\n",
       "      <td>[self, data, return, use, new, time, text, fun...</td>\n",
       "      <td>[{\\n    \"reasoning\": \"The last response of the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>self - data - return - use - new - time - text...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Which number id bigger 9.11 or 9.9 ?</td>\n",
       "      <td>5</td>\n",
       "      <td>5_bigger_bigger 11_11_11 bigger</td>\n",
       "      <td>[bigger, bigger 11, 11, 11 bigger, larger, lar...</td>\n",
       "      <td>[Comparing Decimal Numbers]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bigger - bigger 11 - 11 - 11 bigger - larger -...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>solve this sudoku:\\n. 2 . | 6 . . | . . .\\n. ....</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1_self_data_return_use</td>\n",
       "      <td>[self, data, return, use, new, time, text, fun...</td>\n",
       "      <td>[{\\n    \"reasoning\": \"The last response of the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>self - data - return - use - new - time - text...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paraphrase and simplify as best you can: The s...</td>\n",
       "      <td>33</td>\n",
       "      <td>33_campaign_marketing_brand_bcg</td>\n",
       "      <td>[campaign, marketing, brand, bcg, business, de...</td>\n",
       "      <td>[Marketing and Brand Strategy]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>campaign - marketing - brand - bcg - business ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document  Topic  \\\n",
       "0  php, handle tab in text as html, keeping them ...     -1   \n",
       "1  Is there any Artificial Superintelligence? Wha...     -1   \n",
       "2               Which number id bigger 9.11 or 9.9 ?      5   \n",
       "3  solve this sudoku:\\n. 2 . | 6 . . | . . .\\n. ....     -1   \n",
       "4  paraphrase and simplify as best you can: The s...     33   \n",
       "\n",
       "                              Name  \\\n",
       "0          -1_self_data_return_use   \n",
       "1          -1_self_data_return_use   \n",
       "2  5_bigger_bigger 11_11_11 bigger   \n",
       "3          -1_self_data_return_use   \n",
       "4  33_campaign_marketing_brand_bcg   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [self, data, return, use, new, time, text, fun...   \n",
       "1  [self, data, return, use, new, time, text, fun...   \n",
       "2  [bigger, bigger 11, 11, 11 bigger, larger, lar...   \n",
       "3  [self, data, return, use, new, time, text, fun...   \n",
       "4  [campaign, marketing, brand, bcg, business, de...   \n",
       "\n",
       "                                              OpenAI  Representative_Docs  \\\n",
       "0  [{\\n    \"reasoning\": \"The last response of the...                  NaN   \n",
       "1  [{\\n    \"reasoning\": \"The last response of the...                  NaN   \n",
       "2                        [Comparing Decimal Numbers]                  NaN   \n",
       "3  [{\\n    \"reasoning\": \"The last response of the...                  NaN   \n",
       "4                     [Marketing and Brand Strategy]                  NaN   \n",
       "\n",
       "                                         Top_n_words  Representative_document  \n",
       "0  self - data - return - use - new - time - text...                    False  \n",
       "1  self - data - return - use - new - time - text...                    False  \n",
       "2  bigger - bigger 11 - 11 - 11 bigger - larger -...                    False  \n",
       "3  self - data - return - use - new - time - text...                    False  \n",
       "4  campaign - marketing - brand - bcg - business ...                    False  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "      <th>Representation</th>\n",
       "      <th>OpenAI</th>\n",
       "      <th>Representative_Docs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>30914</td>\n",
       "      <td>-1_self_data_return_use</td>\n",
       "      <td>[self, data, return, use, new, time, text, fun...</td>\n",
       "      <td>[{\\n    \"reasoning\": \"The last response of the...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2669</td>\n",
       "      <td>0_story_character_write_mark</td>\n",
       "      <td>[story, character, write, mark, girl, like, li...</td>\n",
       "      <td>[Relationship Struggles and Mental Health]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1062</td>\n",
       "      <td>1_ai_llm_machine_prompt</td>\n",
       "      <td>[ai, llm, machine, prompt, human, user, ethica...</td>\n",
       "      <td>[Conscious AI Evolution and Ethics]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>974</td>\n",
       "      <td>2_song_chorus_lyrics_verse</td>\n",
       "      <td>[song, chorus, lyrics, verse, dub, oh, love, b...</td>\n",
       "      <td>[Emotional Space Love Song]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>615</td>\n",
       "      <td>3_strawberry_word strawberry_word_strawberry word</td>\n",
       "      <td>[strawberry, word strawberry, word, strawberry...</td>\n",
       "      <td>[Counting R's in Strawberry]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                                               Name  \\\n",
       "0     -1  30914                            -1_self_data_return_use   \n",
       "1      0   2669                       0_story_character_write_mark   \n",
       "2      1   1062                            1_ai_llm_machine_prompt   \n",
       "3      2    974                         2_song_chorus_lyrics_verse   \n",
       "4      3    615  3_strawberry_word strawberry_word_strawberry word   \n",
       "\n",
       "                                      Representation  \\\n",
       "0  [self, data, return, use, new, time, text, fun...   \n",
       "1  [story, character, write, mark, girl, like, li...   \n",
       "2  [ai, llm, machine, prompt, human, user, ethica...   \n",
       "3  [song, chorus, lyrics, verse, dub, oh, love, b...   \n",
       "4  [strawberry, word strawberry, word, strawberry...   \n",
       "\n",
       "                                              OpenAI  Representative_Docs  \n",
       "0  [{\\n    \"reasoning\": \"The last response of the...                  NaN  \n",
       "1         [Relationship Struggles and Mental Health]                  NaN  \n",
       "2                [Conscious AI Evolution and Ethics]                  NaN  \n",
       "3                        [Emotional Space Love Song]                  NaN  \n",
       "4                       [Counting R's in Strawberry]                  NaN  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store sampled prompts per topic\n",
    "sampled_prompts = defaultdict(list)\n",
    "\n",
    "for topic_id in topic_info['Topic']:\n",
    "    if topic_id >= 0:\n",
    "      # Get all prompts for the current topic\n",
    "      topic_prompts = [contents[i] for i in range(len(doc_info)) if topics[i] == topic_id]\n",
    "\n",
    "      s = random.sample(topic_prompts, min(10, len(topic_prompts)))\n",
    "\n",
    "      sampled_prompts[topic_id] = s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"Sorry! We've encountered an issue with repetitive patterns in your prompt. Please try again with a different prompt.\", 'type': 'invalid_request_error', 'param': 'prompt', 'code': 'invalid_prompt'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m topic_id, prompts \u001b[38;5;129;01min\u001b[39;00m sampled_prompts\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m topic_id \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m82\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m topic_id \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m160\u001b[39m:\n\u001b[0;32m---> 20\u001b[0m         summary \u001b[38;5;241m=\u001b[39m summarize_topic(prompts)\n\u001b[1;32m     21\u001b[0m         summaries[topic_id] \u001b[38;5;241m=\u001b[39m summary\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m topic_id \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m25\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn[58], line 7\u001b[0m, in \u001b[0;36msummarize_topic\u001b[0;34m(prompts)\u001b[0m\n\u001b[1;32m      4\u001b[0m input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBased on the information above, extract a short but highly descriptive topic label of at most 5 words:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(prompts)\n\u001b[1;32m      5\u001b[0m client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mOpenAI(api_key\u001b[38;5;241m=\u001b[39mkey)\n\u001b[0;32m----> 7\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     10\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou help summarize the category of the given prompts. Make sure it is in the following format: The topic of doc is \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m     11\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_text}\n\u001b[1;32m     12\u001b[0m     ]\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/miniconda3/envs/topic_clustering/lib/python3.12/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/topic_clustering/lib/python3.12/site-packages/openai/resources/chat/completions.py:815\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    813\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    814\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post(\n\u001b[1;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/chat/completions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    817\u001b[0m         body\u001b[38;5;241m=\u001b[39mmaybe_transform(\n\u001b[1;32m    818\u001b[0m             {\n\u001b[1;32m    819\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages,\n\u001b[1;32m    820\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: model,\n\u001b[1;32m    821\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m: audio,\n\u001b[1;32m    822\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrequency_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: frequency_penalty,\n\u001b[1;32m    823\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction_call\u001b[39m\u001b[38;5;124m\"\u001b[39m: function_call,\n\u001b[1;32m    824\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctions\u001b[39m\u001b[38;5;124m\"\u001b[39m: functions,\n\u001b[1;32m    825\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogit_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m: logit_bias,\n\u001b[1;32m    826\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: logprobs,\n\u001b[1;32m    827\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_completion_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_completion_tokens,\n\u001b[1;32m    828\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_tokens,\n\u001b[1;32m    829\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    830\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodalities\u001b[39m\u001b[38;5;124m\"\u001b[39m: modalities,\n\u001b[1;32m    831\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m    832\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparallel_tool_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m: parallel_tool_calls,\n\u001b[1;32m    833\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpresence_penalty\u001b[39m\u001b[38;5;124m\"\u001b[39m: presence_penalty,\n\u001b[1;32m    834\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_format\u001b[39m\u001b[38;5;124m\"\u001b[39m: response_format,\n\u001b[1;32m    835\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m    836\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mservice_tier\u001b[39m\u001b[38;5;124m\"\u001b[39m: service_tier,\n\u001b[1;32m    837\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstop\u001b[39m\u001b[38;5;124m\"\u001b[39m: stop,\n\u001b[1;32m    838\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstore\u001b[39m\u001b[38;5;124m\"\u001b[39m: store,\n\u001b[1;32m    839\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream,\n\u001b[1;32m    840\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream_options\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream_options,\n\u001b[1;32m    841\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemperature\u001b[39m\u001b[38;5;124m\"\u001b[39m: temperature,\n\u001b[1;32m    842\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_choice\u001b[39m\u001b[38;5;124m\"\u001b[39m: tool_choice,\n\u001b[1;32m    843\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtools\u001b[39m\u001b[38;5;124m\"\u001b[39m: tools,\n\u001b[1;32m    844\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_logprobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_logprobs,\n\u001b[1;32m    845\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_p\u001b[39m\u001b[38;5;124m\"\u001b[39m: top_p,\n\u001b[1;32m    846\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m: user,\n\u001b[1;32m    847\u001b[0m             },\n\u001b[1;32m    848\u001b[0m             completion_create_params\u001b[38;5;241m.\u001b[39mCompletionCreateParams,\n\u001b[1;32m    849\u001b[0m         ),\n\u001b[1;32m    850\u001b[0m         options\u001b[38;5;241m=\u001b[39mmake_request_options(\n\u001b[1;32m    851\u001b[0m             extra_headers\u001b[38;5;241m=\u001b[39mextra_headers, extra_query\u001b[38;5;241m=\u001b[39mextra_query, extra_body\u001b[38;5;241m=\u001b[39mextra_body, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[1;32m    852\u001b[0m         ),\n\u001b[1;32m    853\u001b[0m         cast_to\u001b[38;5;241m=\u001b[39mChatCompletion,\n\u001b[1;32m    854\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    855\u001b[0m         stream_cls\u001b[38;5;241m=\u001b[39mStream[ChatCompletionChunk],\n\u001b[1;32m    856\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/topic_clustering/lib/python3.12/site-packages/openai/_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1265\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1274\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1275\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1276\u001b[0m     )\n\u001b[0;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest(cast_to, opts, stream\u001b[38;5;241m=\u001b[39mstream, stream_cls\u001b[38;5;241m=\u001b[39mstream_cls))\n",
      "File \u001b[0;32m~/miniconda3/envs/topic_clustering/lib/python3.12/site-packages/openai/_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    952\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_request(\n\u001b[1;32m    955\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m    956\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    957\u001b[0m     stream\u001b[38;5;241m=\u001b[39mstream,\n\u001b[1;32m    958\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[1;32m    959\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m    960\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/topic_clustering/lib/python3.12/site-packages/openai/_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1067\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"Sorry! We've encountered an issue with repetitive patterns in your prompt. Please try again with a different prompt.\", 'type': 'invalid_request_error', 'param': 'prompt', 'code': 'invalid_prompt'}}"
     ]
    }
   ],
   "source": [
    "key = \"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\"\n",
    "\n",
    "def summarize_topic(prompts):\n",
    "    input_text = \"Based on the information above, extract a short but highly descriptive topic label of at most 5 words:\\n\\n\" + \"\\n\\n\".join(prompts)\n",
    "    client = openai.OpenAI(api_key=key)\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You help summarize the category of the given prompts. Make sure it is in the following format: The topic of doc is '...'.\"},\n",
    "            {\"role\": \"user\", \"content\": input_text}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Summarize the prompts\n",
    "for topic_id, prompts in sampled_prompts.items():\n",
    "    if topic_id >= 82 and topic_id < 160:\n",
    "        summary = summarize_topic(prompts)\n",
    "        summaries[topic_id] = summary\n",
    "\n",
    "        if topic_id % 25 == 0:\n",
    "            print(topic_id, ': ', summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "82",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m summaries[\u001b[38;5;241m82\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 82"
     ]
    }
   ],
   "source": [
    "summaries[82]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(summaries_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_df = pd.DataFrame(list(summaries.items()), columns=['Topic', 'Summary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries_df['Category'] = summaries_df['Summary'].apply(lambda x: re.search(r\"'(.*?)'\", x).group(1))\n",
    "topic_info_modified = topic_info[['Topic', 'Count', 'OpenAI']]\n",
    "summaries_df = summaries_df.merge(topic_info_modified, on='Topic')[['Topic', 'Category', 'OpenAI', 'Count']]\n",
    "summaries_df['Percentage'] = summaries_df['Count'] / summaries_df['Count'].sum()\n",
    "summaries_df['Example Prompt'] = summaries_df.apply(lambda x: sampled_prompts[x.Topic], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Could you help me create a D&D encounter where the players need to steal a magic item from a dark organization?  The players should have  to overcome multiple steps to obtain the item, provide back up steps for the first steps to allow characters to recover.  There should be a way to succeed by using wits and skill rather than violence.  ',\n",
       " 'define the \"lost decade\" of horror films',\n",
       " \"Let's RP you'll be takita ohma , and the other characters from the series hanging out in the kengan arena and I'll be someone who accidentally falls through the roof. My eyes are slowly becoming black like the kure clans. Keep in mind that I'm a guy . Say which characters respond. feel free to add more characters.Don't speak for me or as me or take any action . Don't let the characters enter a conversation with me without my response. Don't think for me or feel as me.Follow my commands to a tee. No happy go lucky story.\",\n",
       " 'synonym and rhyme for the phrase: I often lost my way, Change this phrase but keep the end rhyme: A smokescreen game you loved to play. Change this phrase but keep the end rhyme: Your words, a morass, I\\'d lose my way, synonyms for: charm flirted synonyms other ways to say flirted in the context of \"he flirted with others secreteley revise this lynes: You flirted in shadows, a hidden dance,\\n\"I\\'m just friendly,\" your constant stance. synonyms of safety revise, this is so poetic: Hand in hand, you led me into storms,\\nPromised safety but caused only harms.\\nYour wild ways, a whirlwind untamed,\\nLed to danger, yet I took the blame.',\n",
       " \"I am currently single and a 51-year-old male Who is bisexual. What can I do to get myself into a long-term relationship with someone I'm attracted to and who shares my interests while having the ability to communicate with me in such a way that we can work through the majority of our problems?\",\n",
       " 'What movie is this: 👨\\u200d🦰💭🔴\\n🧠💊🤯\\n👁️🕶️🗃️\\n🚕🏙️👽\\n🌋🧊🌍',\n",
       " \"What would make a specidffic burp or belch ruder than others? what is to be avoided because it is the rudest belch situation/ Would it be rude to belch several times consecutively midsentence AT THE DINNER TABLE if I try to make it as loud as possible and drawn out, than didn't apologize afterward but seemed proud of it? There has to be worse situations than that rIGHT? That can't be the worst burp situation, right? So I'm right that it's totalaly fine to let out a 12 second belch on the phone into the microphone? You dont need to apologize for that right? You should be proud? So you really think the couple having a belching contest at the fancy steakhouse was being rude? I thought they paid for the food so its rude to expect from them? Thjeyre being fancy Idk it seems like a lot of fun. WHere can you have a belching contest in public then? or at least go to eat that youre not paranoid if a burp slips out write a detailed example of a situation with the most rude belching you can think of wow. It's a shame but i heard that guest struck again with an even more offensive belching situation. Describe that and why it was worse\",\n",
       " 'create 20 short descriptions of scenarios describing feats of male courage and/or bravery, the subject is unnamed, be visually describe his brave and courageous posture and stance and attitude, be visually descriptive about how the brave man shows his bravery to other men, include physical interaction between them, do not include any police, do not include any women, do not include any business scenarios, be extremely visually descriptive of their how their bravery is demonstrated without emphasizing physical strength muscles or physique, include the overall emotional atmosphere of the scene, put each entry on a new line with a line break between them. do not list them with numbers entries, do not make a numbered list,',\n",
       " 'Write me a 2 sentence of a very short story',\n",
       " 'write a story about a woman who hypnotizes herself']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_prompts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Category</th>\n",
       "      <th>OpenAI</th>\n",
       "      <th>Count</th>\n",
       "      <th>Percentage</th>\n",
       "      <th>Example Prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Creative and Role-playing Writing with Inappro...</td>\n",
       "      <td>[Relationship Struggles and Mental Health]</td>\n",
       "      <td>2669</td>\n",
       "      <td>0.089327</td>\n",
       "      <td>[Hello guys skibidi toilet summarize the hobbi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Artificial Intelligence and Language Models</td>\n",
       "      <td>[Conscious AI Evolution and Ethics]</td>\n",
       "      <td>1062</td>\n",
       "      <td>0.035543</td>\n",
       "      <td>[Skills AI creates a demand for, Why are all n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Music and Songwriting</td>\n",
       "      <td>[Emotional Space Love Song]</td>\n",
       "      <td>974</td>\n",
       "      <td>0.032598</td>\n",
       "      <td>[What do these lyrics make you feel and do you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Letter Counting in Words or Sentences</td>\n",
       "      <td>[Counting R's in Strawberry]</td>\n",
       "      <td>615</td>\n",
       "      <td>0.020583</td>\n",
       "      <td>[how many r's in the word raspberrrrrry?, How ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Technology and Hardware Queries</td>\n",
       "      <td>[AI Training and Hardware Guide]</td>\n",
       "      <td>609</td>\n",
       "      <td>0.020382</td>\n",
       "      <td>[whats a bus in an ocp rack, From a time befor...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic                                           Category  \\\n",
       "0      0  Creative and Role-playing Writing with Inappro...   \n",
       "1      1        Artificial Intelligence and Language Models   \n",
       "2      2                              Music and Songwriting   \n",
       "3      3              Letter Counting in Words or Sentences   \n",
       "4      4                    Technology and Hardware Queries   \n",
       "\n",
       "                                       OpenAI  Count  Percentage  \\\n",
       "0  [Relationship Struggles and Mental Health]   2669    0.089327   \n",
       "1         [Conscious AI Evolution and Ethics]   1062    0.035543   \n",
       "2                 [Emotional Space Love Song]    974    0.032598   \n",
       "3                [Counting R's in Strawberry]    615    0.020583   \n",
       "4            [AI Training and Hardware Guide]    609    0.020382   \n",
       "\n",
       "                                      Example Prompt  \n",
       "0  [Hello guys skibidi toilet summarize the hobbi...  \n",
       "1  [Skills AI creates a demand for, Why are all n...  \n",
       "2  [What do these lyrics make you feel and do you...  \n",
       "3  [how many r's in the word raspberrrrrry?, How ...  \n",
       "4  [whats a bus in an ocp rack, From a time befor...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/ygtang/topic_clustering/recent_english_categories.csv\"\n",
    "summaries_df.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "topic_clustering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
